member_name: in-ho

variables:
  pretrained: &pretrained klue/bert-base
  finetuned: &finetuned klue/bert-base # 여기에 save_model 로 저장된 model의 경로를 입력합니다

universal:
  tokenizer:
    train:
      _target_: transformers.AutoTokenizer.from_pretrained
      pretrained_model_name_or_path: *pretrained

    pred:
      _target_: transformers.AutoTokenizer.from_pretrained
      pretrained_model_name_or_path: *finetuned

  train_target_cols:
    - 자격요건
    - 우대조건
    - 복지
    - 회사소개
    - 주요업무
  predict_target_cols:
    - 자격요건
    - 우대조건
    - 복지
    - 회사소개
    - 주요업무

train_data:
  train_data_dir: "../../dataset/pre_result.csv"
  max_batch_size: 12
  train_test_ratio: 0.9
  train_val_ratio: 0.8
  sliding_window_interval: 200
  masked_token_ratio: 0.15
  # max_length: None

predict_data:
  pred_data_dir: "../../JDfeature/sampling_data.csv"
  max_batch_size: 5
  sliding_window_interval: 200

models:
  DefaultBert:
    train:
      _target_: transformers.BertForMaskedLM.from_pretrained
      pretrained_model_name_or_path: *pretrained
    pred:
      _target_: transformers.AutoModel.from_pretrained
      pretrained_model_name_or_path: *finetuned

    mlm:
      _target_: transformers.BertForMaskedLM.from_pretrained
      pretrained_model_name_or_path: *finetuned

models_from_ckpt:
  ckpt_dir: ./checkpoints/v9_s0/last.ckpt
  version_number: 3
  model:
    train:
      _target_: transformers.BertForMaskedLM.from_pretrained
      pretrained_model_name_or_path: *pretrained
    pred:
      _target_: transformers.AutoModel.from_pretrained
      pretrained_model_name_or_path: *finetuned
  name: kcbert-base

task:
  optimizer:
    _target_: "torch.optim.AdamW"
    lr: 2e-5
  lr_scheduler:
    scheduler:
      _target_: "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts"
      T_0: 50
      T_mult: 2
      verbose: False

    interval: epoch

train:
  callbacks:
    checkpoint_callback:
      monitor: "val_avg_loss"
      filename: "best-model-{epoch:02d}-{val_avg_loss:.2f}"
      save_top_k: 1
      save_last: True
      mode: "min"
    early_stop_callback:
      monitor: "val_avg_loss"
      min_delta: 0.01
      patience: 3
      verbose: False
      mode: "min"

  logger:
    # ----- tensorboard -----
    # _target_: pytorch_lightning.loggers.TensorBoardLogger
    # save_dir: tb_logs
    # name: my_model
    # ----- Wandb -----
    name: klue_bert_base
    _target_: pytorch_lightning.loggers.WandbLogger
    project: wandb_test

  trainer:
    max_epochs: 1

is_sweep_enable: False

sweep:
  method: "bayes"
  metric:
    name: "val_avg_loss"
    goal: "minimize"
  parameters:
    max_batch_size:
      values: [4]
  early_terminate:
    type: "hyperband"
    min_iter: 1
  run_cap: 1
