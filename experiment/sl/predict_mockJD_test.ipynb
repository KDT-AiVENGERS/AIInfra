{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similiarity계산 후 유사한 k개 JD뽑기\n",
    "- 가상 JD를 넣으면 inference결과로 embedding vector 생성      **(완료)**  \n",
    "- 다른 JD 와의 column별, 전체 유사도 계산 결과 생성            **(완료)**  \n",
    "- 생성 결과를 추가로 계산하여 가장 유사한 jd 뽑아냄             **```(진행중)```**  \n",
    "-> ```계산방법 논의 필요```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Modules About Hydra\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, LightningDataModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ProgressBar\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT\n",
    "\n",
    "# Modules About Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, BertForMaskedLM, Trainer\n",
    "\n",
    "# Modules About Language Pre-processing\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Others\n",
    "from PIL import Image\n",
    "from typing import List, Any\n",
    "import sys\n",
    "import traceback\n",
    "import yaml\n",
    "import ruamel.yaml\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input\n",
    "data: {'input_ids': [tensor1, tensor2, ...], 'token_type_ids': [tensor1, tensor2, ...], 'attention_mask': [tensor1, tensor2, ...]}\n",
    "'''\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.keys[0]])\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        item = []\n",
    "        for key in self.keys:\n",
    "            item.append(self.data[key][index])\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        train_data_dir=None,\n",
    "        pred_data_dir=None,\n",
    "        max_batch_size=64,\n",
    "        predict_target_cols=[],\n",
    "        train_target_cols=[],\n",
    "        max_length=None,\n",
    "        sliding_window_interval=200,\n",
    "        train_test_ratio=0.9,\n",
    "        train_val_ratio=0.8,\n",
    "        masked_token_ratio=0.15\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.predict_target_cols = predict_target_cols\n",
    "        self.train_target_cols = train_target_cols\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.pred_data_dir = pred_data_dir\n",
    "\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "        self.train_val_ratio = train_val_ratio\n",
    "\n",
    "        self.batch_size = max_batch_size\n",
    "        if predict_target_cols:\n",
    "            self.predict_batch_size = int(\n",
    "                max_batch_size / len(predict_target_cols)) * len(predict_target_cols)\n",
    "\n",
    "        # load Bert Tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if max_length:\n",
    "            self.max_length = max_length\n",
    "        else:\n",
    "            self.max_length = tokenizer.model_max_length\n",
    "\n",
    "        self.sliding_window_interval = sliding_window_interval\n",
    "\n",
    "        self.masked_token_ratio = masked_token_ratio\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # load predict data\n",
    "        try:\n",
    "            self.predict_data_pd = pd.read_csv(\n",
    "                self.pred_data_dir)\n",
    "        except:\n",
    "            print('No inference data available!')\n",
    "            self.predict_data_pd = None\n",
    "\n",
    "        if self.predict_data_pd is not None and self.predict_target_cols:\n",
    "            # serialize columns\n",
    "            predict_data_serialized = []\n",
    "            for row in range(len(self.predict_data_pd)):\n",
    "                for col in self.predict_target_cols:\n",
    "                    predict_data_serialized.append(\n",
    "                        self.predict_data_pd.iloc[row][col])\n",
    "\n",
    "            # make tokens\n",
    "            self.predict_tokens = self.tokenizer(\n",
    "                predict_data_serialized, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # make predict dataset\n",
    "            self.predict_dataset = CustomDataset(self.predict_tokens)\n",
    "            self.predict_token_keys = self.predict_tokens.keys()\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # load train data\n",
    "        try:\n",
    "            self.train_data_pd = pd.read_csv(\n",
    "                self.train_data_dir)\n",
    "        except:\n",
    "            print('No training data available!')\n",
    "            self.train_data_pd = None\n",
    "\n",
    "        if self.train_data_pd is not None and self.train_target_cols:\n",
    "            # serialize columns\n",
    "            train_data_serialized = []\n",
    "            for col in self.train_target_cols:\n",
    "                train_data_serialized += list(self.train_data_pd[col])\n",
    "\n",
    "            # make tokens\n",
    "            self.train_tokens = self.tokenizer(\n",
    "                train_data_serialized, return_tensors='pt', padding=True)\n",
    "            self.train_token_keys = self.train_tokens.keys()\n",
    "\n",
    "            # slicing tokens by a sliding window\n",
    "            current_token_length = self.train_tokens['input_ids'].shape[1]\n",
    "            if current_token_length > self.max_length:\n",
    "                self.train_tokens_sliced = self._make_sliced_tokens(\n",
    "                    self.train_tokens, current_token_length)\n",
    "            else:\n",
    "                self.train_tokens_sliced = self.train_tokens\n",
    "\n",
    "            # make train dataset\n",
    "            train_dataset = CustomDataset(self.train_tokens_sliced)\n",
    "\n",
    "            # split train val test datasets\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "                train_dataset,\n",
    "                [\n",
    "                    self.train_test_ratio * self.train_val_ratio,\n",
    "                    self.train_test_ratio * (1 - self.train_val_ratio),\n",
    "                    1 - self.train_test_ratio\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def _collate_fn_predict(self, batch):\n",
    "        '''\n",
    "        Inputs\n",
    "        batch: [[tensor1_1, tensor1_2, tensor1_3], [tensor2_1, tensor2_2, tensor2_3], ...]\n",
    "        self.predict_token_keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "        Output\n",
    "        dict_by_keys: {'input_ids': [tensor1_1, tensor2_1, ...], 'token_type_ids': [tensor1_2, tensor2_2, ...], 'attention_mask': [tensor1_3, tensor2_3, ...]}\n",
    "        '''\n",
    "        list_by_keys = list(zip(*batch))\n",
    "        dict_by_keys = {}\n",
    "        for i, key in enumerate(self.predict_token_keys):\n",
    "            dict_by_keys[key] = torch.stack(list_by_keys[i])\n",
    "\n",
    "        return dict_by_keys\n",
    "\n",
    "    def _collate_fn_train(self, batch):\n",
    "        list_by_keys = list(zip(*batch))\n",
    "        dict_by_keys = {}\n",
    "        for i, key in enumerate(self.train_token_keys):\n",
    "            dict_by_keys[key] = torch.stack(list_by_keys[i])\n",
    "\n",
    "        dict_by_keys['labels'] = dict_by_keys['input_ids'].clone()\n",
    "\n",
    "        for i, tokens in enumerate(dict_by_keys['input_ids']):\n",
    "            self._make_tokens(\n",
    "                tokens, dict_by_keys['labels'][i], self.masked_token_ratio)\n",
    "\n",
    "        return dict_by_keys\n",
    "\n",
    "    def _make_sliced_tokens(self, tokens, tokens_length):\n",
    "        train_tokens_sliced = {}\n",
    "        for key in self.train_token_keys:\n",
    "            train_tokens_sliced[key] = []\n",
    "\n",
    "        for i in range(len(tokens[key])):\n",
    "            window_index = 0\n",
    "            while True:\n",
    "                if window_index + self.max_length <= tokens_length:\n",
    "                    for key in self.train_token_keys:\n",
    "                        train_tokens_sliced[key].append(\n",
    "                            tokens[key][i][window_index:window_index + self.max_length])\n",
    "\n",
    "                    if tokens[key][i][window_index + self.max_length - 1] != self.tokenizer.pad_token_id:\n",
    "                        window_index += self.sliding_window_interval\n",
    "                        continue\n",
    "                break\n",
    "\n",
    "        return train_tokens_sliced\n",
    "\n",
    "    def _make_tokens(self, tensor1, tensor2, mask_token_ratio, masking_ratio=[0.8, 0.1, 0.1]):\n",
    "        assert sum(masking_ratio) == 1\n",
    "\n",
    "        token_len = 0\n",
    "        for token in tensor1:\n",
    "            if token != self.tokenizer.pad_token_id:\n",
    "                token_len += 1\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        masked_tokens = torch.tensor(np.random.choice(range(\n",
    "            1, token_len - 1), int((token_len - 2) * mask_token_ratio)), dtype=torch.int)\n",
    "        token_types = torch.randint_like(masked_tokens, 1, 101)\n",
    "\n",
    "        tensor2[:] = -100\n",
    "        tensor2[masked_tokens] = tensor1[masked_tokens]\n",
    "        tensor1[masked_tokens] = torch.where(token_types <= int(masking_ratio[0] * 100), self.tokenizer.mask_token_id, torch.where(token_types <= int((1 - masking_ratio[2]) * 100), torch.randint(0, self.tokenizer.vocab_size, (len(masked_tokens),)), tensor1[masked_tokens]))\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_train)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_train)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_train)\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.predict_batch_size, collate_fn=self._collate_fn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertTask(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, optimizer=None, lr_scheduler=None, predict_model=None,\n",
    "                 train_model=None, predict_target_cols=[],\n",
    "                 train_target_cols=[]) -> None:\n",
    "        super().__init__()\n",
    "        self.predict_target_cols = predict_target_cols\n",
    "        self.train_target_cols = train_target_cols\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # self.save_hyperparameters(\n",
    "        #     \"optimizer\", \"tokenizer\",\n",
    "        #     \"lr_scheduler\", \"predict_target_cols\",\n",
    "        #     \"train_target_cols\")\n",
    "\n",
    "        self.predict_model = predict_model\n",
    "        self.train_model = train_model\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.acc_func = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        if not (self.train_model or self.train_target_cols):\n",
    "            print('No train_model or train_target_cols available!')\n",
    "            return\n",
    "\n",
    "        outputs = self.train_model(**batch)\n",
    "\n",
    "        metrics = {\n",
    "            'train_loss': outputs.loss\n",
    "        }\n",
    "        self.training_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT | None:\n",
    "        if not (self.train_model or self.train_target_cols):\n",
    "            print('No train_model or train_target_cols available!')\n",
    "            return\n",
    "\n",
    "        outputs = self.train_model(**batch)\n",
    "\n",
    "        metrics = {\n",
    "            'val_loss': outputs.loss\n",
    "        }\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not (self.training_step_outputs and self.validation_step_outputs):\n",
    "            return\n",
    "\n",
    "        train_avg_loss = torch.stack([x[\"train_loss\"]\n",
    "                                      for x in self.training_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"train_avg_loss\": train_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        val_avg_loss = torch.stack([x[\"val_loss\"]\n",
    "                                    for x in self.validation_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"val_avg_loss\": val_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        print(\"\\n\" +\n",
    "              (f'Epoch {self.current_epoch}, Avg. Training Loss: {train_avg_loss:.3f} ' +\n",
    "               f'Avg. Validation Loss: {val_avg_loss:.3f}'), flush=True)\n",
    "\n",
    "        self.training_step_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        if not (self.train_model or self.train_target_cols):\n",
    "            print('No train_model or train_target_cols available!')\n",
    "            return\n",
    "\n",
    "        outputs = self.train_model(**batch)\n",
    "\n",
    "        metrics = {\n",
    "            'test_loss': outputs.loss\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        sentence_index, mask_token_index = (\n",
    "            batch['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        predicted_token_id = []\n",
    "        for index, sentence in enumerate(sentence_index):\n",
    "            if sentence >= len(predicted_token_id):\n",
    "                predicted_token_id.append([])\n",
    "\n",
    "            predicted_token_id[-1].append(self.tokenizer.decode(\n",
    "                outputs.logits[sentence, mask_token_index[index]].argmax(axis=-1)))\n",
    "\n",
    "        random_numbers = torch.randint(low=0, high=len(\n",
    "            predicted_token_id), size=(int(len(batch['input_ids']) / 2),))\n",
    "\n",
    "        original_token_id = self.tokenizer.batch_decode(batch['input_ids'])\n",
    "\n",
    "        for i in random_numbers:\n",
    "            print(predicted_token_id[i], original_token_id[i])\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx: int = 0):\n",
    "        if not (self.predict_model or self.predict_target_cols):\n",
    "            print('No predict_model or predict_target_cols available!')\n",
    "        outputs = self.predict_model(**batch)\n",
    "        # these are the sentence embedding vectors (768 dim each)\n",
    "        pooler_outputs = outputs[1]\n",
    "        outputs_concated = []\n",
    "        for i in range(int(len(pooler_outputs) / len(self.predict_target_cols))):\n",
    "            outputs_concated.append(torch.concat(list(\n",
    "                pooler_outputs[i * len(self.predict_target_cols):(i + 1) * len(self.predict_target_cols)])))\n",
    "            # Concatenating sentence embedding vectors from a job description\n",
    "\n",
    "        return torch.stack(outputs_concated)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer\n",
    "        if self.lr_scheduler is not None:\n",
    "            return [optimizer], [self.lr_scheduler]\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./huggingface/kcbert_large_2epoch were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./huggingface/kcbert_large_2epoch and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training data available!\n",
      "Predicting DataLoader 0: 100%|██████████| 41/41 [00:12<00:00,  3.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.9855, 0.3175, 0.9118,  ..., 0.9905, 0.9966, 0.9900],\n",
       "         [0.9750, 0.5082, 0.9775,  ..., 0.9976, 0.9941, 0.9851],\n",
       "         [0.9717, 0.3061, 0.9589,  ..., 0.9412, 0.9963, 0.9707],\n",
       "         ...,\n",
       "         [0.6947, 0.8666, 0.6332,  ..., 0.9643, 0.9226, 0.5185],\n",
       "         [0.8408, 0.8370, 0.5661,  ..., 0.9975, 0.9975, 0.9514],\n",
       "         [0.9617, 0.6414, 0.9750,  ..., 0.9587, 0.9478, 0.8475]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./\"):\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "\n",
    "predict_target_cols = cfg.universal.predict_target_cols\n",
    "tokenizer = instantiate(cfg.universal.tokenizer.pred)\n",
    "data_module = HFBertDataModule(\n",
    "    **cfg.predict_data,\n",
    "    tokenizer=tokenizer,\n",
    "    predict_target_cols=predict_target_cols\n",
    ")\n",
    "\n",
    "models = [(instantiate(cfg.models[model].mlm), instantiate(cfg.models[model].pred))\n",
    "          for model in dir(cfg.models)]\n",
    "\n",
    "final_vector_list = []\n",
    "for model in models:\n",
    "    mlm_model = model[0]\n",
    "    predict_model = model[1]\n",
    "    predict_model.pooler.dense = mlm_model.cls.predictions.transform.dense\n",
    "    task = HFBertTask(tokenizer=tokenizer, train_model=mlm_model,\n",
    "                      predict_model=predict_model,\n",
    "                      predict_target_cols=predict_target_cols)\n",
    "\n",
    "    trainer = pl.Trainer()\n",
    "\n",
    "    # this list contains tensors of each output of batch running\n",
    "    predicted_embedding_vectors = trainer.predict(\n",
    "        task, datamodule=data_module)\n",
    "    concatenated_embedding_vectors = torch.concat(\n",
    "        predicted_embedding_vectors, dim=-2)\n",
    "    final_vector_list.append(concatenated_embedding_vectors)\n",
    "final_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./huggingface/kcbert_large_2epoch were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./huggingface/kcbert_large_2epoch and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training data available!\n",
      "Predicting DataLoader 0: 100%|██████████| 41/41 [00:12<00:00,  3.28it/s]\n",
      "✅ predict 결과가 동일합니다!\n"
     ]
    }
   ],
   "source": [
    "# Inference_check : 위의 것과 똑같은지 확인하기\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./\"):\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "\n",
    "predict_target_cols = cfg.universal.predict_target_cols\n",
    "tokenizer = instantiate(cfg.universal.tokenizer.pred)\n",
    "data_module = HFBertDataModule(\n",
    "    **cfg.predict_data,\n",
    "    tokenizer=tokenizer,\n",
    "    predict_target_cols=predict_target_cols\n",
    ")\n",
    "\n",
    "models = [(instantiate(cfg.models[model].mlm), instantiate(cfg.models[model].pred))\n",
    "          for model in dir(cfg.models)]\n",
    "\n",
    "final_vector_list_check = []\n",
    "for model in models:\n",
    "    mlm_model = model[0]\n",
    "    predict_model = model[1]\n",
    "    predict_model.pooler.dense = mlm_model.cls.predictions.transform.dense\n",
    "    task = HFBertTask(tokenizer=tokenizer, train_model=mlm_model,\n",
    "                      predict_model=predict_model,\n",
    "                      predict_target_cols=predict_target_cols)\n",
    "\n",
    "    trainer = pl.Trainer()\n",
    "\n",
    "    # this list contains tensors of each output of batch running\n",
    "    predicted_embedding_vectors = trainer.predict(\n",
    "        task, datamodule=data_module)\n",
    "    concatenated_embedding_vectors = torch.concat(\n",
    "        predicted_embedding_vectors, dim=-2)\n",
    "    final_vector_list_check.append(concatenated_embedding_vectors)\n",
    "if torch.all(final_vector_list[0] == final_vector_list_check[0]):\n",
    "    print(\"✅ predict 결과가 동일합니다!\")\n",
    "else :\n",
    "    print(\"❌ predict 결과가 다릅니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성 vector 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.9855, 0.3175, 0.9118,  ..., 0.9905, 0.9966, 0.9900],\n",
       "         [0.9750, 0.5082, 0.9775,  ..., 0.9976, 0.9941, 0.9851],\n",
       "         [0.9717, 0.3061, 0.9589,  ..., 0.9412, 0.9963, 0.9707],\n",
       "         ...,\n",
       "         [0.6947, 0.8666, 0.6332,  ..., 0.9643, 0.9226, 0.5185],\n",
       "         [0.8408, 0.8370, 0.5661,  ..., 0.9975, 0.9975, 0.9514],\n",
       "         [0.9617, 0.6414, 0.9750,  ..., 0.9587, 0.9478, 0.8475]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 5120])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vector_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_vector_list[0],'kclarge2_sample.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 JD 와의 column별, 전체 유사도 계산 결과 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가상 JD와 가장 유사한 K개의 원본 JD뽑아내기\n",
    "#추가작업필요\n",
    "\n",
    "def Top_k_JD(vec_mock: torch.tensor, origin_JD_csv, vec_origin: torch.tensor, k):\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(vec_mock.unsqueeze(0),vec_origin, dim=1).argsort(descending=True, dim =0)\n",
    "\n",
    "    return origin_JD_csv.loc[cosine_sim.tolist()[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가상 JD와 column별 cosine similarity DataFrame 생성\n",
    "\n",
    "def Cosine_Sim_Col(vec_mock: torch.tensor, vec_origin: torch.tensor):\n",
    "    \n",
    "    col_name = ['자격요건','우대조건','복지','회사소개','주요업무','전체']\n",
    "    cosine_dic = {}\n",
    "\n",
    "    origins = torch.chunk(vec_origin, chunks=5, dim=1)\n",
    "    mocks = torch.chunk(vec_mock, chunks=5, dim=0)\n",
    "\n",
    "    cosine_dic['전체'] = pd.Series(torch.nn.functional.cosine_similarity(vec_mock.unsqueeze(0),vec_origin, dim=1))\n",
    "\n",
    "    for origin, mock, col in zip(origins, mocks,col_name):\n",
    "        cosine_dic[col]  = pd.Series(torch.nn.functional.cosine_similarity(mock.unsqueeze(0),origin, dim=1))\n",
    "\n",
    "    return pd.DataFrame(cosine_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터  \n",
    "mock_JD = ```QnA를 통해 사용자에게 얻은 가상 jd의 embedding_vector data```  \n",
    "origin_JD_csv = ```서버에 올라간 jd전체 data```  \n",
    "origin_JD_em = ```서버에 올라간 jd전체 embedding_vector data```   \n",
    "k = ```최대 k개의 데이터를 뽑음```\n",
    "  \n",
    "- cosine 추가 계산 방법 논의  \n",
    "-> ```합계, 나누기, 평균, 가중치 ...```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 코드:\n",
    "origin_JD_csv = pd.read_csv('../../dataset/pre_result.csv')\n",
    "origin_JD_em = torch.load('kclarge2_sample.pth')\n",
    "mock_JD = origin_JD_em[:][0]\n",
    "\n",
    "\n",
    "#cosine 유사도 dataframe 생성\n",
    "cosine_df = Cosine_Sim_Col(mock_JD, origin_JD_em)\n",
    "\n",
    "#Top k개 공고 출력\n",
    "# JD_list = Top_k_JD(mock_JD, origin_JD_csv, origin_JD_em, k)\n",
    "# JD_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전체</th>\n",
       "      <th>자격요건</th>\n",
       "      <th>우대조건</th>\n",
       "      <th>복지</th>\n",
       "      <th>회사소개</th>\n",
       "      <th>주요업무</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995529</td>\n",
       "      <td>0.995498</td>\n",
       "      <td>0.997423</td>\n",
       "      <td>0.997143</td>\n",
       "      <td>0.991397</td>\n",
       "      <td>0.996710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.959888</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>0.930170</td>\n",
       "      <td>0.996645</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>0.994484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.987310</td>\n",
       "      <td>0.994324</td>\n",
       "      <td>0.996085</td>\n",
       "      <td>0.994374</td>\n",
       "      <td>0.995708</td>\n",
       "      <td>0.957762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971447</td>\n",
       "      <td>0.929232</td>\n",
       "      <td>0.993530</td>\n",
       "      <td>0.997176</td>\n",
       "      <td>0.995605</td>\n",
       "      <td>0.942725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956741</td>\n",
       "      <td>0.937767</td>\n",
       "      <td>0.994977</td>\n",
       "      <td>0.897395</td>\n",
       "      <td>0.997461</td>\n",
       "      <td>0.952567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.964319</td>\n",
       "      <td>0.932812</td>\n",
       "      <td>0.993328</td>\n",
       "      <td>0.897715</td>\n",
       "      <td>0.995384</td>\n",
       "      <td>0.996261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.901509</td>\n",
       "      <td>0.913314</td>\n",
       "      <td>0.920894</td>\n",
       "      <td>0.869809</td>\n",
       "      <td>0.996247</td>\n",
       "      <td>0.798138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.985314</td>\n",
       "      <td>0.991976</td>\n",
       "      <td>0.993962</td>\n",
       "      <td>0.997730</td>\n",
       "      <td>0.997639</td>\n",
       "      <td>0.946985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992250</td>\n",
       "      <td>0.985995</td>\n",
       "      <td>0.994934</td>\n",
       "      <td>0.989149</td>\n",
       "      <td>0.996831</td>\n",
       "      <td>0.994672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.995823</td>\n",
       "      <td>0.993721</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>0.994490</td>\n",
       "      <td>0.994778</td>\n",
       "      <td>0.998923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.973642</td>\n",
       "      <td>0.990584</td>\n",
       "      <td>0.951214</td>\n",
       "      <td>0.994029</td>\n",
       "      <td>0.995076</td>\n",
       "      <td>0.938029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.995625</td>\n",
       "      <td>0.992210</td>\n",
       "      <td>0.997197</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.995740</td>\n",
       "      <td>0.998252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.956249</td>\n",
       "      <td>0.943511</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.997910</td>\n",
       "      <td>0.955981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.996098</td>\n",
       "      <td>0.995601</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.994433</td>\n",
       "      <td>0.994717</td>\n",
       "      <td>0.998848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.990491</td>\n",
       "      <td>0.994547</td>\n",
       "      <td>0.993937</td>\n",
       "      <td>0.970307</td>\n",
       "      <td>0.997237</td>\n",
       "      <td>0.995613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.972686</td>\n",
       "      <td>0.918106</td>\n",
       "      <td>0.996853</td>\n",
       "      <td>0.951666</td>\n",
       "      <td>0.997768</td>\n",
       "      <td>0.996108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.996307</td>\n",
       "      <td>0.993804</td>\n",
       "      <td>0.995476</td>\n",
       "      <td>0.996877</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>0.997819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.948887</td>\n",
       "      <td>0.991930</td>\n",
       "      <td>0.860902</td>\n",
       "      <td>0.926270</td>\n",
       "      <td>0.994870</td>\n",
       "      <td>0.963185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.935701</td>\n",
       "      <td>0.993575</td>\n",
       "      <td>0.906811</td>\n",
       "      <td>0.895894</td>\n",
       "      <td>0.996716</td>\n",
       "      <td>0.880399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.958746</td>\n",
       "      <td>0.993062</td>\n",
       "      <td>0.995013</td>\n",
       "      <td>0.848230</td>\n",
       "      <td>0.994371</td>\n",
       "      <td>0.953398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.972280</td>\n",
       "      <td>0.989672</td>\n",
       "      <td>0.945302</td>\n",
       "      <td>0.992909</td>\n",
       "      <td>0.998322</td>\n",
       "      <td>0.936913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.983971</td>\n",
       "      <td>0.992563</td>\n",
       "      <td>0.995643</td>\n",
       "      <td>0.996504</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>0.949450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.958746</td>\n",
       "      <td>0.993062</td>\n",
       "      <td>0.995013</td>\n",
       "      <td>0.848230</td>\n",
       "      <td>0.994371</td>\n",
       "      <td>0.953398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.985639</td>\n",
       "      <td>0.994334</td>\n",
       "      <td>0.941465</td>\n",
       "      <td>0.996704</td>\n",
       "      <td>0.997202</td>\n",
       "      <td>0.997673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.986413</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.995701</td>\n",
       "      <td>0.951067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.969339</td>\n",
       "      <td>0.911261</td>\n",
       "      <td>0.987162</td>\n",
       "      <td>0.993554</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>0.956912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.995610</td>\n",
       "      <td>0.993335</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>0.994521</td>\n",
       "      <td>0.996313</td>\n",
       "      <td>0.997691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.996852</td>\n",
       "      <td>0.992619</td>\n",
       "      <td>0.995299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.968375</td>\n",
       "      <td>0.993483</td>\n",
       "      <td>0.946760</td>\n",
       "      <td>0.912506</td>\n",
       "      <td>0.989970</td>\n",
       "      <td>0.995080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.993560</td>\n",
       "      <td>0.873162</td>\n",
       "      <td>0.839205</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>0.942878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.983793</td>\n",
       "      <td>0.995753</td>\n",
       "      <td>0.943950</td>\n",
       "      <td>0.992038</td>\n",
       "      <td>0.992450</td>\n",
       "      <td>0.993894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.994720</td>\n",
       "      <td>0.991932</td>\n",
       "      <td>0.996117</td>\n",
       "      <td>0.994746</td>\n",
       "      <td>0.993966</td>\n",
       "      <td>0.997232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.992633</td>\n",
       "      <td>0.995845</td>\n",
       "      <td>0.997294</td>\n",
       "      <td>0.994925</td>\n",
       "      <td>0.993818</td>\n",
       "      <td>0.982305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.986101</td>\n",
       "      <td>0.995641</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>0.947801</td>\n",
       "      <td>0.996752</td>\n",
       "      <td>0.992218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.985910</td>\n",
       "      <td>0.991495</td>\n",
       "      <td>0.994990</td>\n",
       "      <td>0.995637</td>\n",
       "      <td>0.996559</td>\n",
       "      <td>0.953032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.903118</td>\n",
       "      <td>0.755826</td>\n",
       "      <td>0.930509</td>\n",
       "      <td>0.994709</td>\n",
       "      <td>0.996327</td>\n",
       "      <td>0.816805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.973298</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.994996</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.997399</td>\n",
       "      <td>0.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.950612</td>\n",
       "      <td>0.960594</td>\n",
       "      <td>0.996251</td>\n",
       "      <td>0.838262</td>\n",
       "      <td>0.995496</td>\n",
       "      <td>0.952092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.981413</td>\n",
       "      <td>0.921491</td>\n",
       "      <td>0.994832</td>\n",
       "      <td>0.994185</td>\n",
       "      <td>0.997726</td>\n",
       "      <td>0.997423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.933870</td>\n",
       "      <td>0.991748</td>\n",
       "      <td>0.934998</td>\n",
       "      <td>0.893966</td>\n",
       "      <td>0.902106</td>\n",
       "      <td>0.944126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          전체      자격요건      우대조건        복지      회사소개      주요업무\n",
       "0   1.000000  1.000000  1.000000  1.000000  1.000000  1.000000\n",
       "1   0.995529  0.995498  0.997423  0.997143  0.991397  0.996710\n",
       "2   0.959888  0.994700  0.930170  0.996645  0.881122  0.994484\n",
       "3   0.987310  0.994324  0.996085  0.994374  0.995708  0.957762\n",
       "4   0.971447  0.929232  0.993530  0.997176  0.995605  0.942725\n",
       "5   0.956741  0.937767  0.994977  0.897395  0.997461  0.952567\n",
       "6   0.964319  0.932812  0.993328  0.897715  0.995384  0.996261\n",
       "7   0.901509  0.913314  0.920894  0.869809  0.996247  0.798138\n",
       "8   0.985314  0.991976  0.993962  0.997730  0.997639  0.946985\n",
       "9   0.992250  0.985995  0.994934  0.989149  0.996831  0.994672\n",
       "10  0.995823  0.993721  0.997218  0.994490  0.994778  0.998923\n",
       "11  0.973642  0.990584  0.951214  0.994029  0.995076  0.938029\n",
       "12  0.995625  0.992210  0.997197  0.995142  0.995740  0.998252\n",
       "13  0.956249  0.943511  0.960764  0.921053  0.997910  0.955981\n",
       "14  0.996098  0.995601  0.997002  0.994433  0.994717  0.998848\n",
       "15  0.990491  0.994547  0.993937  0.970307  0.997237  0.995613\n",
       "16  0.972686  0.918106  0.996853  0.951666  0.997768  0.996108\n",
       "17  0.996307  0.993804  0.995476  0.996877  0.997506  0.997819\n",
       "18  0.948887  0.991930  0.860902  0.926270  0.994870  0.963185\n",
       "19  0.935701  0.993575  0.906811  0.895894  0.996716  0.880399\n",
       "20  0.958746  0.993062  0.995013  0.848230  0.994371  0.953398\n",
       "21  0.972280  0.989672  0.945302  0.992909  0.998322  0.936913\n",
       "22  0.983971  0.992563  0.995643  0.996504  0.988279  0.949450\n",
       "23  0.958746  0.993062  0.995013  0.848230  0.994371  0.953398\n",
       "24  0.985639  0.994334  0.941465  0.996704  0.997202  0.997673\n",
       "25  0.986413  0.996078  0.996860  0.994582  0.995701  0.951067\n",
       "26  0.969339  0.911261  0.987162  0.993554  0.997561  0.956912\n",
       "27  0.995610  0.993335  0.996180  0.994521  0.996313  0.997691\n",
       "28  0.994996  0.993750  0.996815  0.996852  0.992619  0.995299\n",
       "29  0.968375  0.993483  0.946760  0.912506  0.989970  0.995080\n",
       "30  0.931818  0.993560  0.873162  0.839205  0.998147  0.942878\n",
       "31  0.983793  0.995753  0.943950  0.992038  0.992450  0.993894\n",
       "32  0.994720  0.991932  0.996117  0.994746  0.993966  0.997232\n",
       "33  0.992633  0.995845  0.997294  0.994925  0.993818  0.982305\n",
       "34  0.986101  0.995641  0.996669  0.947801  0.996752  0.992218\n",
       "35  0.985910  0.991495  0.994990  0.995637  0.996559  0.953032\n",
       "36  0.903118  0.755826  0.930509  0.994709  0.996327  0.816805\n",
       "37  0.973298  0.914556  0.994996  0.992197  0.997399  0.967000\n",
       "38  0.950612  0.960594  0.996251  0.838262  0.995496  0.952092\n",
       "39  0.981413  0.921491  0.994832  0.994185  0.997726  0.997423\n",
       "40  0.933870  0.991748  0.934998  0.893966  0.902106  0.944126"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiinfra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
