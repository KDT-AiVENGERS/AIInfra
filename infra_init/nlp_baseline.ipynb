{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Example\n",
    "## 혐오표현 분류\n",
    "\n",
    "### 사용 방법\n",
    "1. 데이터 다운로드: https://www.kaggle.com/competitions/kdtai-2/data 에서 train.csv 와 test.csv 다운로드\n",
    "2. 데이터 폴더: root/data/Discrimination 폴더에 다운받은 두 csv 파일 위치\n",
    "3. Mecab 이 설치되어 있다면 그냥 진행. 설치되어 있지 않다면 Mecab 설치 항목 진행\n",
    "\n",
    "### Mecab 설치 방법\n",
    "1. 터미널에서 root/settings 폴더로 이동\n",
    "2. (중요!) 가상환경 진입\n",
    "3. bash install_mecab.sh 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab 이 잘 설치되었는지 확인하기 위해 사용해 주시기 바랍니다.\n",
    "\n",
    "# import re\n",
    "# from konlpy.tag import Mecab\n",
    "\n",
    "# def preprocess_korean_text(self, text):\n",
    "#     # Remove URLs and mentions\n",
    "#     text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "#     text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "#     # Tokenize text using Mecab\n",
    "#     mecab = Mecab()\n",
    "#     tokens = mecab.morphs(text)\n",
    "\n",
    "#     # Remove stop words (optional)\n",
    "#     stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "#     tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "#     # Remove punctuation and non-Korean characters\n",
    "#     tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "#     tokens = [t for t in tokens if t]\n",
    "\n",
    "#     return tokens\n",
    "\n",
    "# preprocess_korean_text('안녕 이건 테스트야')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules About Hydra\n",
    "# from tqdm.notebook import tqdm\n",
    "# from PIL import Image\n",
    "from typing import List, Any\n",
    "# from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Modules About Language Pre-processing\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class for korean text\n",
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, word_key_to_index, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.word_key_to_index = word_key_to_index\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.word_key_to_index:\n",
    "                indices.append(self.word_key_to_index[token])\n",
    "            else:\n",
    "                indices.append(self.word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        if not np.isnan(label):\n",
    "            return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "        return torch.tensor(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS\n",
    "\n",
    "\n",
    "class SequentialDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 64, data_dir: str = '../data/') -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.transform = None\n",
    "\n",
    "        self.max_length = 200\n",
    "\n",
    "        # load train data\n",
    "        self.train_data = pd.read_csv(self.data_dir + 'Discrimination/train.csv')\n",
    "\n",
    "        # make word map from train data\n",
    "        print('Making word map')\n",
    "        self._make_word_map(self.train_data)\n",
    "\n",
    "    # custom function for preprocess korean text\n",
    "    def _preprocess_korean_text(self, text):\n",
    "        # Remove URLs and mentions\n",
    "        text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "        text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "        # Tokenize text using Mecab\n",
    "        mecab = Mecab()\n",
    "        tokens = mecab.morphs(text)\n",
    "\n",
    "        # Remove stop words (optional)\n",
    "        stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "        # Remove punctuation and non-Korean characters\n",
    "        tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "        tokens = [t for t in tokens if t]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # custom function for making word map\n",
    "    def _make_word_map(self, train_data):\n",
    "        self.word_index_to_key = []\n",
    "        self.word_key_to_index = {}\n",
    "\n",
    "        for i in range(len(train_data)):\n",
    "            text = train_data.iloc[i]['text']\n",
    "            tokens = self._preprocess_korean_text(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in self.word_key_to_index:\n",
    "                    self.word_key_to_index[token] = len(self.word_index_to_key)\n",
    "                    self.word_index_to_key.append(token)\n",
    "\n",
    "        self.word_key_to_index['<unk>'] = len(self.word_index_to_key)\n",
    "        self.word_index_to_key.append('<unk>')\n",
    "\n",
    "    # just write for downloading actions\n",
    "    def prepare_data(self) -> None:\n",
    "        # load predict data\n",
    "        self.predict_data = pd.read_csv(self.data_dir + 'Discrimination/test.csv')\n",
    "\n",
    "        # make predict dataset\n",
    "        self.predict_dataset = KoreanTextDataset(\n",
    "            data=self.predict_data,\n",
    "            word_key_to_index=self.word_key_to_index,\n",
    "            preprocess_korean_text=self._preprocess_korean_text,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == 'fit':\n",
    "            train_test_ratio = 0.9\n",
    "            train_val_ratio = 0.8\n",
    "\n",
    "            # make train dataset\n",
    "            train_dataset = KoreanTextDataset(\n",
    "                data=self.train_data,\n",
    "                word_key_to_index=self.word_key_to_index,\n",
    "                preprocess_korean_text=self._preprocess_korean_text,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "\n",
    "            # split train val test dataset\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "                train_dataset, [train_test_ratio * train_val_ratio, train_test_ratio * (1 - train_val_ratio), 1 - train_test_ratio]\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom attention class\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom rnn_lstm_attention class\n",
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=LSTM_layers, bidirectional=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        lstm_outputs, (hidden, _) = self.lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        # h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        h = hidden[-1,:,:]\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = self.attention(h, lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(pl.LightningModule):\n",
    "    def __init__(self, word_index_to_key, config=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # define loss function\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # define model\n",
    "        self.model = RNN_LSTM_attention(\n",
    "            vocab_size=len(word_index_to_key),\n",
    "            embedding_dim=100,\n",
    "            hidden_dim=512,\n",
    "            output_dim=7,\n",
    "            LSTM_layers=2,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # etc custom attributes\n",
    "        self.idx_to_class = {\n",
    "            0: 'Origin(출신차별)',\n",
    "            1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "            2: 'Politics(정치성향차별)',\n",
    "            3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "            4: 'Age(연령차별)',\n",
    "            5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "            6: 'Not Hate Speech(해당사항없음)',\n",
    "        }\n",
    "\n",
    "    def forward(self, x, y=None) -> Any:\n",
    "        output = self.model(x)\n",
    "\n",
    "        if y is not None:\n",
    "            loss = self.loss_func(output, y)\n",
    "            return loss, output\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SequentialTask(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # define accuracy function\n",
    "        self.acc_func = Accuracy(\n",
    "            task='multiclass',\n",
    "            num_classes=7\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'train_acc': acc,\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        self.training_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'val_acc': acc,\n",
    "            'val_loss': loss,\n",
    "        }\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not (self.training_step_outputs and self.validation_step_outputs):\n",
    "            return\n",
    "\n",
    "        train_avg_loss = torch.stack([x[\"train_loss\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        train_avg_acc = torch.stack([x[\"train_acc\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"train_avg_acc\": train_avg_acc,\n",
    "            \"train_avg_loss\": train_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        val_avg_loss = torch.stack([x[\"val_loss\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        val_avg_acc = torch.stack([x[\"val_acc\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"val_avg_acc\": val_avg_acc,\n",
    "            \"val_avg_loss\": val_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        print(\"\\n\" +\n",
    "              (f'Epoch {self.current_epoch}, Avg. Training Loss: {train_avg_loss:.3f}, Avg. Training Accuracy: {train_avg_acc:.3f} ' +\n",
    "               f'Avg. Validation Loss: {val_avg_loss:.3f}, Avg. Validation Accuracy: {val_avg_acc:.3f}'), flush=True)\n",
    "\n",
    "        self.training_step_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'test_acc': acc,\n",
    "            'test_loss': loss,\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:\n",
    "        return torch.argmax(self.model(batch), dim=-1)\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making word map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | SequentialModel    | 7.6 M \n",
      "1 | acc_func | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "3.7 M     Non-trainable params\n",
      "7.6 M     Total params\n",
      "30.249    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1482/1482 [02:58<00:00,  8.30it/s, v_num=29, train_acc=0.767, train_loss=0.880]\n",
      "Epoch 0, Avg. Training Loss: 1.178, Avg. Training Accuracy: 0.602 Avg. Validation Loss: 0.881, Avg. Validation Accuracy: 0.709\n",
      "Epoch 1: 100%|██████████| 1482/1482 [02:56<00:00,  8.39it/s, v_num=29, train_acc=0.733, train_loss=0.653, val_acc=0.713, val_loss=0.875]\n",
      "Epoch 1, Avg. Training Loss: 0.833, Avg. Training Accuracy: 0.722 Avg. Validation Loss: 0.756, Avg. Validation Accuracy: 0.748\n",
      "Epoch 2: 100%|██████████| 1482/1482 [02:56<00:00,  8.41it/s, v_num=29, train_acc=0.633, train_loss=1.230, val_acc=0.748, val_loss=0.757]\n",
      "Epoch 2, Avg. Training Loss: 0.706, Avg. Training Accuracy: 0.762 Avg. Validation Loss: 0.728, Avg. Validation Accuracy: 0.750\n",
      "Epoch 3: 100%|██████████| 1482/1482 [02:52<00:00,  8.57it/s, v_num=29, train_acc=0.767, train_loss=0.495, val_acc=0.750, val_loss=0.729]\n",
      "Epoch 3, Avg. Training Loss: 0.607, Avg. Training Accuracy: 0.792 Avg. Validation Loss: 0.700, Avg. Validation Accuracy: 0.758\n",
      "Epoch 4: 100%|██████████| 1482/1482 [02:52<00:00,  8.61it/s, v_num=29, train_acc=0.767, train_loss=0.783, val_acc=0.758, val_loss=0.700]\n",
      "Epoch 4, Avg. Training Loss: 0.522, Avg. Training Accuracy: 0.819 Avg. Validation Loss: 0.726, Avg. Validation Accuracy: 0.755\n",
      "Epoch 5: 100%|██████████| 1482/1482 [02:53<00:00,  8.56it/s, v_num=29, train_acc=0.767, train_loss=0.880, val_acc=0.755, val_loss=0.727]\n",
      "Epoch 5, Avg. Training Loss: 0.447, Avg. Training Accuracy: 0.844 Avg. Validation Loss: 0.748, Avg. Validation Accuracy: 0.760\n",
      "Epoch 6: 100%|██████████| 1482/1482 [02:52<00:00,  8.59it/s, v_num=29, train_acc=0.667, train_loss=0.811, val_acc=0.760, val_loss=0.749] \n",
      "Epoch 6, Avg. Training Loss: 0.382, Avg. Training Accuracy: 0.865 Avg. Validation Loss: 0.765, Avg. Validation Accuracy: 0.762\n",
      "Epoch 6: 100%|██████████| 1482/1482 [03:13<00:00,  7.64it/s, v_num=29, train_acc=0.667, train_loss=0.811, val_acc=0.762, val_loss=0.765]\n",
      "Testing DataLoader 0: 100%|██████████| 206/206 [00:11<00:00, 17.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7663224935531616     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7920665740966797     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7663224935531616    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7920665740966797    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.7663224935531616, 'test_loss': 0.7920665740966797}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = SequentialDataModule(batch_size=32)\n",
    "\n",
    "model = SequentialModel(data_module.word_index_to_key)\n",
    "task = SequentialTask(model)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_acc',\n",
    "    save_top_k=3,\n",
    "    mode='max'\n",
    "))\n",
    "\n",
    "callbacks.append(EarlyStopping(\n",
    "    monitor='val_avg_acc',\n",
    "    min_delta=0.01,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='max',\n",
    "    # stopping_threshold=,\n",
    "    # divergence_threshold=,\n",
    "    # check_finite=,\n",
    "    # check_on_train_epoch_end=,\n",
    "))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.fit(model=task, datamodule=data_module)\n",
    "trainer.test(model=task, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making word map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 422/422 [00:24<00:00, 17.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = SequentialDataModule(batch_size=32)\n",
    "\n",
    "model = SequentialModel(data_module.word_index_to_key)\n",
    "task = SequentialTask.load_from_checkpoint('./lightning_logs/version_29/checkpoints/epoch=6-step=10374.ckpt', model=model)\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "predicted_classes = trainer.predict(task, datamodule=data_module)\n",
    "concated_predicted_classes = torch.concat(predicted_classes, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_text</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"늙더니 뇌가 망가져 버렸나보네 ㅉㅉ 하긴 옛날부터 개소리 참 잘했어\"</td>\n",
       "      <td>Age(연령차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"계집년 구하러다 죄없는 군인만 죽네\"</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"달창들 조국 줫되게 생겼으니까 깔 기사가 없어서 여기로 몰려오겠구나~~~~\"</td>\n",
       "      <td>Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대깨문들은 존중해 줄 필요가 없는 족속들인데, 어짜피 욕 먹을거 그냥 문재앙이라고 하지.</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>전현무가 너무 아까웠지. 못생기고 골 텅텅 비고 성격 드세고... 대체 왜 만났는지.</td>\n",
       "      <td>Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>평화는 싫어하면서 단체 앞에 자유만 갖다붙히는 극우 친일당과 추종자들은 고대로 일본...</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"함박도나찾아라 조센징들\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>효리네민박을 이경규호텔로 바꾼거네...매회 연예인 초대해서</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"정권은 항상 삽질 ㅂㅅ 짓하고 국민은 열심히 그걸 이겨냈지.\"</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>노홍철이 설마 실수로 음주운전했겠냐 그전에 뭐가 있었는데? 유세윤 셀프자숙하려고 음...</td>\n",
       "      <td>Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>트와이스 어디갔노??</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>힘들때 웃는 자가 일류랬는데 이 새키 초일류네 ㅋㅋㅋㅋㅋ</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>일진 왕따설은 사실인가?전국민이 다아는 대스타의 자식이고 아픔 공감해주니 오버하고 ...</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>대깨문은 쓰레기다</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>문지상은 카메오로 등장한다고 합니다</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"자빠구리 먹고 배탈났냐?\"</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>선미 욕하지마 오크년들아. 선미가 얼마나 예쁜데 욕을 하냐?ㅉㅉㅉ</td>\n",
       "      <td>Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>의사들이 인정했는데 니들이 뭔데 시비임 니들이 의사야?</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"가만히보니까 참 희한하게 생겼네 아베 ㅋㅋ\"</td>\n",
       "      <td>Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>주니엘은 반전 가창력은 아니지 원래 노래로 승부보는 가수였는데 비교하긴 좀 그렇지만...</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sample_text  \\\n",
       "0             \"늙더니 뇌가 망가져 버렸나보네 ㅉㅉ 하긴 옛날부터 개소리 참 잘했어\"   \n",
       "1                               \"계집년 구하러다 죄없는 군인만 죽네\"   \n",
       "2         \"달창들 조국 줫되게 생겼으니까 깔 기사가 없어서 여기로 몰려오겠구나~~~~\"   \n",
       "3   대깨문들은 존중해 줄 필요가 없는 족속들인데, 어짜피 욕 먹을거 그냥 문재앙이라고 하지.   \n",
       "4     전현무가 너무 아까웠지. 못생기고 골 텅텅 비고 성격 드세고... 대체 왜 만났는지.   \n",
       "5   평화는 싫어하면서 단체 앞에 자유만 갖다붙히는 극우 친일당과 추종자들은 고대로 일본...   \n",
       "6                                      \"함박도나찾아라 조센징들\"   \n",
       "7                    효리네민박을 이경규호텔로 바꾼거네...매회 연예인 초대해서   \n",
       "8                 \"정권은 항상 삽질 ㅂㅅ 짓하고 국민은 열심히 그걸 이겨냈지.\"   \n",
       "9   노홍철이 설마 실수로 음주운전했겠냐 그전에 뭐가 있었는데? 유세윤 셀프자숙하려고 음...   \n",
       "10                                        트와이스 어디갔노??   \n",
       "11                    힘들때 웃는 자가 일류랬는데 이 새키 초일류네 ㅋㅋㅋㅋㅋ   \n",
       "12  일진 왕따설은 사실인가?전국민이 다아는 대스타의 자식이고 아픔 공감해주니 오버하고 ...   \n",
       "13                                          대깨문은 쓰레기다   \n",
       "14                                문지상은 카메오로 등장한다고 합니다   \n",
       "15                                    \"자빠구리 먹고 배탈났냐?\"   \n",
       "16               선미 욕하지마 오크년들아. 선미가 얼마나 예쁜데 욕을 하냐?ㅉㅉㅉ   \n",
       "17                     의사들이 인정했는데 니들이 뭔데 시비임 니들이 의사야?   \n",
       "18                          \"가만히보니까 참 희한하게 생겼네 아베 ㅋㅋ\"   \n",
       "19  주니엘은 반전 가창력은 아니지 원래 노래로 승부보는 가수였는데 비교하긴 좀 그렇지만...   \n",
       "\n",
       "                                      predicted_class  \n",
       "0                                           Age(연령차별)  \n",
       "1   Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "2       Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.  \n",
       "3                                    Politics(정치성향차별)  \n",
       "4       Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.  \n",
       "5                                    Politics(정치성향차별)  \n",
       "6                                        Origin(출신차별)  \n",
       "7                             Not Hate Speech(해당사항없음)  \n",
       "8   Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "9       Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.  \n",
       "10                            Not Hate Speech(해당사항없음)  \n",
       "11  Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "12                            Not Hate Speech(해당사항없음)  \n",
       "13                                   Politics(정치성향차별)  \n",
       "14                                   Politics(정치성향차별)  \n",
       "15                            Not Hate Speech(해당사항없음)  \n",
       "16      Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.  \n",
       "17                            Not Hate Speech(해당사항없음)  \n",
       "18      Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.  \n",
       "19                            Not Hate Speech(해당사항없음)  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx = np.random.randint(low=0, high=len(concated_predicted_classes) - 1, size=20)\n",
    "\n",
    "sample_text = []\n",
    "predicted_class = []\n",
    "\n",
    "for i in rand_idx:\n",
    "    sample_text.append(data_module.predict_data.loc[i, 'text'])\n",
    "    predicted_class.append(model.idx_to_class[concated_predicted_classes[i].item()])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'sample_text': sample_text,\n",
    "    'predicted_class': predicted_class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiinfra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
