{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to prepare your data for inference task\n",
    "1. make data.csv file in root/data/JobDescription/ folder\n",
    "* data.csv has columns of JD sections\n",
    "* data.csv has rows of JDs\n",
    "* data.csv has cells of sentences\n",
    "2. change 'target_cols' list before running\n",
    "3. run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/aiinfra/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Modules About Hydra\n",
    "\n",
    "# from PIL import Image\n",
    "from typing import List, Any\n",
    "# from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT\n",
    "\n",
    "# Modules About Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Modules About Language Pre-processing\n",
    "# import re\n",
    "# from konlpy.tag import Mecab\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input\n",
    "data: {'input_ids': [tensor1, tensor2, ...], 'token_type_ids': [tensor1, tensor2, ...], 'attention_mask': [tensor1, tensor2, ...]}\n",
    "'''\n",
    "\n",
    "class CustomDatasetForInference(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.keys[0]])\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        item = []\n",
    "        for key in self.keys:\n",
    "            item.append(self.data[key][index])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, target_cols, pretrained_model_name, max_batch_size=64, data_dir='../data/') -> None:\n",
    "        super().__init__()\n",
    "        self.target_cols = target_cols\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.batch_size = int(max_batch_size / len(target_cols)) * len(target_cols)\n",
    "\n",
    "        # load Bert Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # load predict data\n",
    "        self.predict_data_pd = pd.read_csv(self.data_dir + 'JobDescription/data.csv')\n",
    "\n",
    "        # serialize columns\n",
    "        predict_data_serialized = []\n",
    "        for row in range(len(self.predict_data_pd)):\n",
    "            for col in self.target_cols:\n",
    "                predict_data_serialized.append(self.predict_data_pd.iloc[row][col])\n",
    "\n",
    "        # make tokens\n",
    "        self.tokens = self.tokenizer(predict_data_serialized, return_tensors='pt', padding=True)\n",
    "\n",
    "        # make predict dataset\n",
    "        self.predict_dataset = CustomDatasetForInference(self.tokens)\n",
    "        self.predict_token_keys = self.predict_dataset.keys\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        pass\n",
    "\n",
    "    def _collate_fn_predict(self, batch):\n",
    "        '''\n",
    "        Inputs\n",
    "        batch: [[tensor1_1, tensor1_2, tensor1_3], [tensor2_1, tensor2_2, tensor2_3], ...]\n",
    "        self.predict_token_keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "        Output\n",
    "        dict_by_keys: {'input_ids': [tensor1_1, tensor2_1, ...], 'token_type_ids': [tensor1_2, tensor2_2, ...], 'attention_mask': [tensor1_3, tensor2_3, ...]}\n",
    "        '''\n",
    "        list_by_keys = list(zip(*batch))\n",
    "        dict_by_keys = {}\n",
    "        for i, key in enumerate(self.predict_token_keys):\n",
    "            dict_by_keys[key] = torch.stack(list_by_keys[i])\n",
    "\n",
    "        return dict_by_keys\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertModel(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model_name, config=None) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "\n",
    "        self.loss_func = None\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model_name, output_hidden_states=False)\n",
    "\n",
    "    def forward(self, x, y=None) -> Any:\n",
    "        output = self.model(**x) # put arguments by **kwargs method\n",
    "\n",
    "        if y is not None:\n",
    "            loss = self.loss_func(output, y)\n",
    "            return loss, output\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertTask(pl.LightningModule):\n",
    "    def __init__(self, model, n_target_cols) -> None:\n",
    "        super().__init__()\n",
    "        self.n_target_cols = n_target_cols\n",
    "\n",
    "        self.model = model\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.acc_func = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT | None:\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> STEP_OUTPUT | None:\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx: int = 0):\n",
    "        outputs = self.model(batch)\n",
    "        pooler_outputs = outputs['pooler_output'] # these are the sentence embedding vectors (768 dim each)\n",
    "        outputs_concated = []\n",
    "        for i in range(int(len(pooler_outputs) / self.n_target_cols)):\n",
    "            outputs_concated.append(torch.concat(list(pooler_outputs[i * self.n_target_cols:(i + 1) * self.n_target_cols])))\n",
    "            # Concatenating sentence embedding vectors from a job description\n",
    "\n",
    "        return torch.stack(outputs_concated)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['sentences']\n",
    "\n",
    "data_module = HFBertDataModule(\n",
    "    target_cols=target_cols,\n",
    "    pretrained_model_name='bert-base-multilingual-cased',\n",
    "    max_batch_size=64,\n",
    ")\n",
    "model = HFBertModel('bert-base-multilingual-cased')\n",
    "task = HFBertTask(model, len(target_cols))\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "predicted_embedding_vectors = trainer.predict(task, datamodule=data_module) # this list contains tensors of each output of batch running\n",
    "concatenated_embedding_vectors = torch.concat(predicted_embedding_vectors, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_embedding_vectors.shape # number of sentence * embedding vector dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiinfra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
