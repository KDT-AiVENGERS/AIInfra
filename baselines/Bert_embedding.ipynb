{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to prepare your data for inference task\n",
    "1. make data.csv file in root/data/JobDescription/ folder\n",
    "* data.csv has columns of JD sections\n",
    "* data.csv has rows of JDs\n",
    "* data.csv has cells of sentences\n",
    "2. change 'target_cols' list before running\n",
    "3. run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/aiinfra/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Modules About Hydra\n",
    "# from PIL import Image\n",
    "from typing import List, Any\n",
    "# from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT\n",
    "\n",
    "# Modules About Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, BertForMaskedLM, Trainer\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Modules About Language Pre-processing\n",
    "# import re\n",
    "# from konlpy.tag import Mecab\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input\n",
    "data: {'input_ids': [tensor1, tensor2, ...], 'token_type_ids': [tensor1, tensor2, ...], 'attention_mask': [tensor1, tensor2, ...]}\n",
    "'''\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.keys[0]])\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        item = []\n",
    "        for key in self.keys:\n",
    "            item.append(self.data[key][index])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, max_batch_size=64, data_dir='../data/', predict_target_cols=[], train_target_cols=[], train_test_ratio=0.9, train_val_ratio=0.8) -> None:\n",
    "        super().__init__()\n",
    "        self.predict_target_cols = predict_target_cols\n",
    "        self.train_target_cols = train_target_cols\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "        self.train_val_ratio = train_val_ratio\n",
    "\n",
    "        self.batch_size = max_batch_size\n",
    "        if predict_target_cols:\n",
    "            self.predict_batch_size = int(max_batch_size / len(predict_target_cols)) * len(predict_target_cols)\n",
    "\n",
    "        # load Bert Tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # load predict data\n",
    "        try:\n",
    "            self.predict_data_pd = pd.read_csv(self.data_dir + 'JobDescription/inference_data.csv')\n",
    "        except:\n",
    "            print('No inference data available!')\n",
    "\n",
    "        if self.predict_data_pd is not None and self.predict_target_cols:\n",
    "            # serialize columns\n",
    "            predict_data_serialized = []\n",
    "            for row in range(len(self.predict_data_pd)):\n",
    "                for col in self.predict_target_cols:\n",
    "                    predict_data_serialized.append(self.predict_data_pd.iloc[row][col])\n",
    "\n",
    "            # make tokens\n",
    "            self.predict_tokens = self.tokenizer(predict_data_serialized, return_tensors='pt', padding=True)\n",
    "\n",
    "            # make predict dataset\n",
    "            self.predict_dataset = CustomDataset(self.predict_tokens)\n",
    "            self.predict_token_keys = self.predict_dataset.keys\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # load train data\n",
    "        try:\n",
    "            self.train_data_pd = pd.read_csv(self.data_dir + 'JobDescription/training_data.csv')\n",
    "        except:\n",
    "            print('No training data available!')\n",
    "            self.train_data_pd = None\n",
    "\n",
    "        if self.train_data_pd is not None and self.train_target_cols:\n",
    "            # serialize columns\n",
    "            train_data_serialized = []\n",
    "            for col in self.train_target_cols:\n",
    "                train_data_serialized += list(self.train_data_pd[col])\n",
    "\n",
    "            # make tokens\n",
    "            self.train_tokens = self.tokenizer(train_data_serialized, return_tensors='pt', padding=True)\n",
    "\n",
    "            # make train dataset\n",
    "            train_dataset = CustomDataset(self.train_tokens)\n",
    "            self.train_token_keys = train_dataset.keys\n",
    "\n",
    "            # split train val test datasets\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "                train_dataset,\n",
    "                [\n",
    "                    self.train_test_ratio * self.train_val_ratio,\n",
    "                    self.train_test_ratio * (1 - self.train_val_ratio),\n",
    "                    1 - self.train_test_ratio\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def _collate_fn_predict(self, batch):\n",
    "        '''\n",
    "        Inputs\n",
    "        batch: [[tensor1_1, tensor1_2, tensor1_3], [tensor2_1, tensor2_2, tensor2_3], ...]\n",
    "        self.predict_token_keys: ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "        Output\n",
    "        dict_by_keys: {'input_ids': [tensor1_1, tensor2_1, ...], 'token_type_ids': [tensor1_2, tensor2_2, ...], 'attention_mask': [tensor1_3, tensor2_3, ...]}\n",
    "        '''\n",
    "        list_by_keys = list(zip(*batch))\n",
    "        dict_by_keys = {}\n",
    "        for i, key in enumerate(self.predict_token_keys):\n",
    "            dict_by_keys[key] = torch.stack(list_by_keys[i])\n",
    "\n",
    "        return dict_by_keys\n",
    "\n",
    "    def _collate_fn_train(self, batch):\n",
    "        list_by_keys = list(zip(*batch))\n",
    "        dict_by_keys = {}\n",
    "        for i, key in enumerate(self.train_token_keys):\n",
    "            dict_by_keys[key] = torch.stack(list_by_keys[i])\n",
    "\n",
    "        dict_by_keys['labels'] = dict_by_keys['input_ids'].clone()\n",
    "\n",
    "        for tokens in dict_by_keys['input_ids']:\n",
    "            token_len = 0\n",
    "            for token in tokens:\n",
    "                if token != self.tokenizer.pad_token_id:\n",
    "                    token_len += 1\n",
    "                    continue\n",
    "                break\n",
    "            tokens[np.random.randint(1, token_len - 2)] = self.tokenizer.mask_token_id\n",
    "\n",
    "        dict_by_keys['labels'] = torch.where(dict_by_keys['input_ids'] == self.tokenizer.mask_token_id, dict_by_keys['labels'], -100)\n",
    "\n",
    "        return dict_by_keys\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_train)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_train)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self._collate_fn_train)\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.predict_batch_size, collate_fn=self._collate_fn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFBertTask(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, predict_model=None, train_model=None, predict_target_cols=[], train_target_cols=[]) -> None:\n",
    "        super().__init__()\n",
    "        self.predict_target_cols = predict_target_cols\n",
    "        self.train_target_cols = train_target_cols\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.predict_model = predict_model\n",
    "        self.train_model = train_model\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.acc_func = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        if not (self.train_model or self.train_target_cols):\n",
    "            print('No train_model or train_target_cols available!')\n",
    "            return\n",
    "\n",
    "        outputs = self.train_model(**batch)\n",
    "\n",
    "        metrics = {\n",
    "            'train_loss': outputs.loss\n",
    "        }\n",
    "        self.training_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT | None:\n",
    "        if not (self.train_model or self.train_target_cols):\n",
    "            print('No train_model or train_target_cols available!')\n",
    "            return\n",
    "\n",
    "        outputs = self.train_model(**batch)\n",
    "\n",
    "        metrics = {\n",
    "            'val_loss': outputs.loss\n",
    "        }\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not (self.training_step_outputs and self.validation_step_outputs):\n",
    "            return\n",
    "\n",
    "        train_avg_loss = torch.stack([x[\"train_loss\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"train_avg_loss\": train_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        val_avg_loss = torch.stack([x[\"val_loss\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"val_avg_loss\": val_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        print(\"\\n\" +\n",
    "              (f'Epoch {self.current_epoch}, Avg. Training Loss: {train_avg_loss:.3f} ' +\n",
    "               f'Avg. Validation Loss: {val_avg_loss:.3f}'), flush=True)\n",
    "\n",
    "        self.training_step_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        if not (self.train_model or self.train_target_cols):\n",
    "            print('No train_model or train_target_cols available!')\n",
    "            return\n",
    "\n",
    "        outputs = self.train_model(**batch)\n",
    "\n",
    "        metrics = {\n",
    "            'test_loss': outputs.loss\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        sentence_index, mask_token_index = (batch['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        predicted_token_id = []\n",
    "        for index, sentence in enumerate(sentence_index):\n",
    "            if sentence >= len(predicted_token_id):\n",
    "                predicted_token_id.append([])\n",
    "\n",
    "            predicted_token_id[-1].append(self.tokenizer.decode(outputs.logits[sentence, mask_token_index[index]].argmax(axis=-1)))\n",
    "\n",
    "        random_numbers = torch.randint(low=0, high=len(predicted_token_id), size=(20,))\n",
    "\n",
    "        original_token_id = self.tokenizer.batch_decode(batch['input_ids'])\n",
    "\n",
    "        for i in random_numbers:\n",
    "            print(predicted_token_id[i], original_token_id[i])\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx: int = 0):\n",
    "        if not (self.predict_model or self.predict_target_cols):\n",
    "            print('No predict_model or predict_target_cols available!')\n",
    "\n",
    "        outputs = self.predict_model(**batch)\n",
    "        pooler_outputs = outputs['pooler_output'] # these are the sentence embedding vectors (768 dim each)\n",
    "        outputs_concated = []\n",
    "        for i in range(int(len(pooler_outputs) / len(self.predict_target_cols))):\n",
    "            outputs_concated.append(torch.concat(list(pooler_outputs[i * len(self.predict_target_cols):(i + 1) * len(self.predict_target_cols)])))\n",
    "            # Concatenating sentence embedding vectors from a job description\n",
    "\n",
    "        return torch.stack(outputs_concated)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/tglim/Programmers_AI_dev/KDT-AiVENGERS/AIInfra/baselines/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "predict_target_cols = ['sentences']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "data_module = HFBertDataModule(\n",
    "    tokenizer=tokenizer,\n",
    "    max_batch_size=64,\n",
    "    predict_target_cols=predict_target_cols,\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "task = HFBertTask(tokenizer=tokenizer, predict_model=model, predict_target_cols=predict_target_cols)\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "predicted_embedding_vectors = trainer.predict(task, datamodule=data_module) # this list contains tensors of each output of batch running\n",
    "concatenated_embedding_vectors = torch.concat(predicted_embedding_vectors, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the inference result\n",
    "\n",
    "concatenated_embedding_vectors.shape # number of sentence * embedding vector dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/tglim/Programmers_AI_dev/KDT-AiVENGERS/AIInfra/baselines/lightning_logs\n",
      "\n",
      "  | Name        | Type            | Params\n",
      "------------------------------------------------\n",
      "0 | train_model | BertForMaskedLM | 177 M \n",
      "------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.898   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s, v_num=0, train_loss=0.0702]\n",
      "Epoch 0, Avg. Training Loss: 0.070 Avg. Validation Loss: 0.053\n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  5.94it/s, v_num=0, train_loss=0.0829, val_loss=0.0431]\n",
      "Epoch 1, Avg. Training Loss: 0.083 Avg. Validation Loss: 0.008\n",
      "Testing DataLoader 0:   0%|          | 0/1 [28:34<?, ?it/s]m=0, train_loss=0.0829, val_loss=0.00806]\n",
      "Testing DataLoader 0:   0%|          | 0/1 [28:03<?, ?it/s]\n",
      "Testing DataLoader 0:   0%|          | 0/1 [27:14<?, ?it/s]\n",
      "Testing DataLoader 0:   0%|          | 0/1 [09:26<?, ?it/s]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s, v_num=0, train_loss=0.0665, val_loss=0.00806]\n",
      "Epoch 2, Avg. Training Loss: 0.067 Avg. Validation Loss: 0.064\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s, v_num=0, train_loss=0.125, val_loss=0.0638]  \n",
      "Epoch 3, Avg. Training Loss: 0.125 Avg. Validation Loss: 0.076\n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  5.84it/s, v_num=0, train_loss=0.0752, val_loss=0.0756]\n",
      "Epoch 4, Avg. Training Loss: 0.075 Avg. Validation Loss: 0.045\n",
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  6.53it/s, v_num=0, train_loss=0.197, val_loss=0.0448] \n",
      "Epoch 5, Avg. Training Loss: 0.197 Avg. Validation Loss: 0.026\n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s, v_num=0, train_loss=0.0486, val_loss=0.0255]\n",
      "Epoch 6, Avg. Training Loss: 0.049 Avg. Validation Loss: 0.033\n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  6.68it/s, v_num=0, train_loss=0.0742, val_loss=0.0327]\n",
      "Epoch 7, Avg. Training Loss: 0.074 Avg. Validation Loss: 0.041\n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s, v_num=0, train_loss=0.0973, val_loss=0.0408]\n",
      "Epoch 8, Avg. Training Loss: 0.097 Avg. Validation Loss: 0.030\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  6.71it/s, v_num=0, train_loss=0.0788, val_loss=0.0296]\n",
      "Epoch 9, Avg. Training Loss: 0.079 Avg. Validation Loss: 0.045\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s, v_num=0, train_loss=0.0788, val_loss=0.0448]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, v_num=0, train_loss=0.0788, val_loss=0.0448]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "train_target_cols = ['sentences']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "data_module = HFBertDataModule(\n",
    "    tokenizer=tokenizer,\n",
    "    max_batch_size=64,\n",
    "    train_target_cols=train_target_cols,\n",
    "    train_test_ratio=0.7,\n",
    "    train_val_ratio=0.5\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "task = HFBertTask(tokenizer=tokenizer, train_model=model, train_target_cols=train_target_cols)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "trainer.fit(task, datamodule=data_module) # this list contains tensors of each output of batch running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['-'] [CLS] 2. 여러분은 각 문장이 어떻게 Embedding [MASK] 로 변환되는지 test 해보실 수 있으십니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['.'] [CLS] 5 [MASK] Example sentence 는 5개면 충분하겠지요? 너무 예시가 많아도 불편하니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10760138183832169    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10760138183832169   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.10760138183832169}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "trainer.test(task, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hugging face model & tokenizer weights\n",
    "\n",
    "hf_trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "hf_trainer.save_model('../models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiinfra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
