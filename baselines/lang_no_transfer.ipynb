{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Example\n",
    "## 혐오표현 분류\n",
    "\n",
    "### 사용 방법\n",
    "1. 데이터 다운로드: https://www.kaggle.com/competitions/kdtai-2/data 에서 train.csv 와 test.csv 다운로드\n",
    "2. 데이터 폴더: root/data/Discrimination 폴더에 다운받은 두 csv 파일 위치\n",
    "3. Mecab 이 설치되어 있다면 그냥 진행. 설치되어 있지 않다면 Mecab 설치 항목 진행\n",
    "\n",
    "### Mecab 설치 방법\n",
    "1. 터미널에서 root/settings 폴더로 이동\n",
    "2. (중요!) 가상환경 진입\n",
    "3. bash install_mecab.sh 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "def preprocess_korean_text(self, text):\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional)\n",
    "    stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "preprocess_korean_text('안녕 이건 테스트야')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/aiinfra/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Modules About Hydra\n",
    "# from tqdm.notebook import tqdm\n",
    "# from PIL import Image\n",
    "from typing import List, Any\n",
    "# from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Modules About Language Pre-processing\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class for korean text\n",
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, word_key_to_index, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.word_key_to_index = word_key_to_index\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.word_key_to_index:\n",
    "                indices.append(self.word_key_to_index[token])\n",
    "            else:\n",
    "                indices.append(self.word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        if not np.isnan(label):\n",
    "            return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "        return torch.tensor(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS\n",
    "\n",
    "\n",
    "class SequentialDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 64, data_dir: str = '../data/') -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.transform = None\n",
    "\n",
    "        self.max_length = 200\n",
    "\n",
    "        # load train data\n",
    "        self.train_data = pd.read_csv(self.data_dir + 'Discrimination/train.csv')\n",
    "\n",
    "        # load predict data\n",
    "        self.predict_data = pd.read_csv(self.data_dir + 'Discrimination/test.csv')\n",
    "\n",
    "        # make word map from train data\n",
    "        print('Making word map')\n",
    "        self._make_word_map(self.train_data)\n",
    "\n",
    "    # custom function for preprocess korean text\n",
    "    def _preprocess_korean_text(self, text):\n",
    "        # Remove URLs and mentions\n",
    "        text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "        text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "        # Tokenize text using Mecab\n",
    "        mecab = Mecab()\n",
    "        tokens = mecab.morphs(text)\n",
    "\n",
    "        # Remove stop words (optional)\n",
    "        stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "        # Remove punctuation and non-Korean characters\n",
    "        tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "        tokens = [t for t in tokens if t]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # custom function for making word map\n",
    "    def _make_word_map(self, train_data):\n",
    "        self.word_index_to_key = []\n",
    "        self.word_key_to_index = {}\n",
    "\n",
    "        for i in range(len(train_data)):\n",
    "            text = train_data.iloc[i]['text']\n",
    "            tokens = self._preprocess_korean_text(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in self.word_key_to_index:\n",
    "                    self.word_key_to_index[token] = len(self.word_index_to_key)\n",
    "                    self.word_index_to_key.append(token)\n",
    "\n",
    "        self.word_key_to_index['<unk>'] = len(self.word_index_to_key)\n",
    "        self.word_index_to_key.append('<unk>')\n",
    "\n",
    "    # just write for downloading actions\n",
    "    def prepare_data(self) -> None:\n",
    "        # make predict dataset\n",
    "        self.predict_dataset = KoreanTextDataset(\n",
    "            data=self.predict_data,\n",
    "            word_key_to_index=self.word_key_to_index,\n",
    "            preprocess_korean_text=self._preprocess_korean_text,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == 'fit':\n",
    "            train_test_ratio = 0.9\n",
    "            train_val_ratio = 0.8\n",
    "\n",
    "            # make train dataset\n",
    "            train_dataset = KoreanTextDataset(\n",
    "                data=self.train_data,\n",
    "                word_key_to_index=self.word_key_to_index,\n",
    "                preprocess_korean_text=self._preprocess_korean_text,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "\n",
    "            # split train val test dataset\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "                train_dataset, [train_test_ratio * train_val_ratio, train_test_ratio * (1 - train_val_ratio), 1 - train_test_ratio]\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom attention class\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom rnn_lstm_attention class\n",
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=LSTM_layers, bidirectional=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        lstm_outputs, (hidden, _) = self.lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        # h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        h = hidden[-1,:,:]\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = self.attention(h, lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(pl.LightningModule):\n",
    "    def __init__(self, word_index_to_key, config=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # define loss function\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # define model\n",
    "        self.model = RNN_LSTM_attention(\n",
    "            vocab_size=len(word_index_to_key),\n",
    "            embedding_dim=100,\n",
    "            hidden_dim=512,\n",
    "            output_dim=7,\n",
    "            LSTM_layers=2,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # etc custom attributes\n",
    "        self.idx_to_class = {\n",
    "            0: 'Origin(출신차별)',\n",
    "            1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "            2: 'Politics(정치성향차별)',\n",
    "            3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "            4: 'Age(연령차별)',\n",
    "            5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "            6: 'Not Hate Speech(해당사항없음)',\n",
    "        }\n",
    "\n",
    "    def forward(self, x, y=None) -> Any:\n",
    "        output = self.model(x)\n",
    "        if y:\n",
    "            loss = self.loss_func(output, y)\n",
    "            return loss, output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SequentialTask(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # define accuracy function\n",
    "        self.acc_func = Accuracy(\n",
    "            task='multiclass',\n",
    "            num_classes=7\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'train_acc': acc,\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        self.training_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'val_acc': acc,\n",
    "            'val_loss': loss,\n",
    "        }\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not (self.training_step_outputs and self.validation_step_outputs):\n",
    "            return\n",
    "\n",
    "        train_avg_loss = torch.stack([x[\"train_loss\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        train_avg_acc = torch.stack([x[\"train_acc\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"train_avg_acc\": train_avg_acc,\n",
    "            \"train_avg_loss\": train_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        val_avg_loss = torch.stack([x[\"val_loss\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        val_avg_acc = torch.stack([x[\"val_acc\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"val_avg_acc\": val_avg_acc,\n",
    "            \"val_avg_loss\": val_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        print(\"\\n\" +\n",
    "              (f'Epoch {self.current_epoch}, Avg. Training Loss: {train_avg_loss:.3f}, Avg. Training Accuracy: {train_avg_acc:.3f} ' +\n",
    "               f'Avg. Validation Loss: {val_avg_loss:.3f}, Avg. Validation Accuracy: {val_avg_acc:.3f}'), flush=True)\n",
    "\n",
    "        self.training_step_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'test_acc': acc,\n",
    "            'test_loss': loss,\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:\n",
    "        return torch.argmax(self.model(batch), dim=-1)\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making word map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | SequentialModel    | 7.6 M \n",
      "1 | acc_func | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "3.7 M     Non-trainable params\n",
      "7.6 M     Total params\n",
      "30.249    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1482/1482 [02:51<00:00,  8.65it/s, v_num=12, train_acc=0.633, train_loss=1.020]\n",
      "Epoch 0, Avg. Training Loss: 1.170, Avg. Training Accuracy: 0.605 Avg. Validation Loss: 0.907, Avg. Validation Accuracy: 0.697\n",
      "Epoch 1: 100%|██████████| 1482/1482 [02:50<00:00,  8.69it/s, v_num=12, train_acc=0.733, train_loss=0.711, val_acc=0.701, val_loss=0.901]\n",
      "Epoch 1, Avg. Training Loss: 0.827, Avg. Training Accuracy: 0.724 Avg. Validation Loss: 0.790, Avg. Validation Accuracy: 0.732\n",
      "Epoch 2: 100%|██████████| 1482/1482 [02:48<00:00,  8.77it/s, v_num=12, train_acc=0.667, train_loss=0.653, val_acc=0.732, val_loss=0.789]\n",
      "Epoch 2, Avg. Training Loss: 0.697, Avg. Training Accuracy: 0.763 Avg. Validation Loss: 0.731, Avg. Validation Accuracy: 0.752\n",
      "Epoch 3: 100%|██████████| 1482/1482 [02:48<00:00,  8.80it/s, v_num=12, train_acc=0.733, train_loss=0.485, val_acc=0.752, val_loss=0.731]\n",
      "Epoch 3, Avg. Training Loss: 0.600, Avg. Training Accuracy: 0.794 Avg. Validation Loss: 0.732, Avg. Validation Accuracy: 0.753\n",
      "Epoch 4: 100%|██████████| 1482/1482 [02:48<00:00,  8.78it/s, v_num=12, train_acc=0.800, train_loss=0.569, val_acc=0.754, val_loss=0.732]\n",
      "Epoch 4, Avg. Training Loss: 0.516, Avg. Training Accuracy: 0.822 Avg. Validation Loss: 0.767, Avg. Validation Accuracy: 0.755\n",
      "Epoch 5: 100%|██████████| 1482/1482 [02:50<00:00,  8.69it/s, v_num=12, train_acc=0.867, train_loss=0.478, val_acc=0.755, val_loss=0.768]\n",
      "Epoch 5, Avg. Training Loss: 0.447, Avg. Training Accuracy: 0.844 Avg. Validation Loss: 0.793, Avg. Validation Accuracy: 0.759\n",
      "Epoch 5: 100%|██████████| 1482/1482 [03:11<00:00,  7.72it/s, v_num=12, train_acc=0.867, train_loss=0.478, val_acc=0.759, val_loss=0.793]\n",
      "Testing DataLoader 0: 100%|██████████| 206/206 [00:10<00:00, 19.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7610082030296326     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.785693883895874     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7610082030296326    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.785693883895874    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.7610082030296326, 'test_loss': 0.785693883895874}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = SequentialDataModule(batch_size=32)\n",
    "\n",
    "model = SequentialModel(data_module.word_index_to_key)\n",
    "task = SequentialTask(model)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_acc',\n",
    "    save_top_k=3,\n",
    "    mode='max'\n",
    "))\n",
    "\n",
    "callbacks.append(EarlyStopping(\n",
    "    monitor='val_avg_acc',\n",
    "    min_delta=0.01,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='max',\n",
    "    # stopping_threshold=,\n",
    "    # divergence_threshold=,\n",
    "    # check_finite=,\n",
    "    # check_on_train_epoch_end=,\n",
    "))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.fit(model=task, datamodule=data_module)\n",
    "trainer.test(model=task, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making word map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 422/422 [00:23<00:00, 18.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = SequentialDataModule(batch_size=32)\n",
    "\n",
    "model = SequentialModel(data_module.word_index_to_key)\n",
    "task = SequentialTask.load_from_checkpoint('./lightning_logs/version_12/checkpoints/epoch=5-step=8892.ckpt', model=model)\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "predicted_classes = trainer.predict(task, datamodule=data_module)\n",
    "concated_predicted_classes = torch.concat(predicted_classes, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_text</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excelsior!</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"보수좀비들임. 아줌마, 틀딱노땅, 일베남자들이 많음\"</td>\n",
       "      <td>Age(연령차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"아직도 5%야? 어떤 친일파놈들이 쪽바리 제품에 ~~~\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"좋아 죽을때는 섹스파트너, 헤어지면 성노리개, 이게 미투냐?\"</td>\n",
       "      <td>Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"쪽빠리당 현정권 아베세끼와 연합하여 파괴시도중\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>박근혜 아니겠 쥐</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"짱깨 조선족 종북주사파들 공통점 - 본인들을 반대하면 기승전 토왜몰이\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"이쯤되면 막 가자는거지요? 뻘게이 새끼들 존나 쳐 맞아야 되겠네~\"</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>와~ 적당히들 좀 해라 10년전 일에 지금와서 티비에 좀 자주보인다고 미투??ㅎㅎㅎ...</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"문재인 노무현 두분다 인복이 없다..저런 쓰래기들을 달고다니니...\"</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>최고의 성형은 다이어트 맞네요.몸관리 잘 하셔서 오래오래 유지하세요.</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>재미있을 거 같은데 왜 다들 난리인지.</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"하여간 전라도 새키들이 문제ㅋㅋ\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"정부 응원 안하는 쪽바리들은 일본으로 가라 너희 아베총가 기다린다\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"입이야 똥구멍이야? ㅋㅋ\"</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>대학가 연극 동아리들도 털어봐라 거기 엄청 물란하고 심각하더라</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"못생긴거 보기싫다. 토나와. 잘생긴 조국 장관님만 볼래용💙💙💙💙💙\"</td>\n",
       "      <td>Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>역시 1화부터 존나 재미없었음 ㅋㅋㅋㅋ</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"저것이 돼도 즐라도와 민주당은 머리 조아리고 딸랑딸랑 하것지? ㅎ\"</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>미국산 쇠고기먹으면 뒤진다고 사기친넘들이다 뭔들 못하겠냐</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sample_text  \\\n",
       "0                                          Excelsior!   \n",
       "1                      \"보수좀비들임. 아줌마, 틀딱노땅, 일베남자들이 많음\"   \n",
       "2                    \"아직도 5%야? 어떤 친일파놈들이 쪽바리 제품에 ~~~\"   \n",
       "3                 \"좋아 죽을때는 섹스파트너, 헤어지면 성노리개, 이게 미투냐?\"   \n",
       "4                         \"쪽빠리당 현정권 아베세끼와 연합하여 파괴시도중\"   \n",
       "5                                           박근혜 아니겠 쥐   \n",
       "6            \"짱깨 조선족 종북주사파들 공통점 - 본인들을 반대하면 기승전 토왜몰이\"   \n",
       "7              \"이쯤되면 막 가자는거지요? 뻘게이 새끼들 존나 쳐 맞아야 되겠네~\"   \n",
       "8   와~ 적당히들 좀 해라 10년전 일에 지금와서 티비에 좀 자주보인다고 미투??ㅎㅎㅎ...   \n",
       "9             \"문재인 노무현 두분다 인복이 없다..저런 쓰래기들을 달고다니니...\"   \n",
       "10             최고의 성형은 다이어트 맞네요.몸관리 잘 하셔서 오래오래 유지하세요.   \n",
       "11                              재미있을 거 같은데 왜 다들 난리인지.   \n",
       "12                                \"하여간 전라도 새키들이 문제ㅋㅋ\"   \n",
       "13             \"정부 응원 안하는 쪽바리들은 일본으로 가라 너희 아베총가 기다린다\"   \n",
       "14                                    \"입이야 똥구멍이야? ㅋㅋ\"   \n",
       "15                 대학가 연극 동아리들도 털어봐라 거기 엄청 물란하고 심각하더라   \n",
       "16             \"못생긴거 보기싫다. 토나와. 잘생긴 조국 장관님만 볼래용💙💙💙💙💙\"   \n",
       "17                              역시 1화부터 존나 재미없었음 ㅋㅋㅋㅋ   \n",
       "18             \"저것이 돼도 즐라도와 민주당은 머리 조아리고 딸랑딸랑 하것지? ㅎ\"   \n",
       "19                    미국산 쇠고기먹으면 뒤진다고 사기친넘들이다 뭔들 못하겠냐   \n",
       "\n",
       "                                      predicted_class  \n",
       "0                             Not Hate Speech(해당사항없음)  \n",
       "1                                           Age(연령차별)  \n",
       "2                                        Origin(출신차별)  \n",
       "3           Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.  \n",
       "4                                        Origin(출신차별)  \n",
       "5                             Not Hate Speech(해당사항없음)  \n",
       "6                                        Origin(출신차별)  \n",
       "7   Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "8                             Not Hate Speech(해당사항없음)  \n",
       "9                                    Politics(정치성향차별)  \n",
       "10                            Not Hate Speech(해당사항없음)  \n",
       "11                            Not Hate Speech(해당사항없음)  \n",
       "12                                       Origin(출신차별)  \n",
       "13                                       Origin(출신차별)  \n",
       "14                            Not Hate Speech(해당사항없음)  \n",
       "15                            Not Hate Speech(해당사항없음)  \n",
       "16      Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.  \n",
       "17  Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "18                                       Origin(출신차별)  \n",
       "19                            Not Hate Speech(해당사항없음)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx = np.random.randint(low=0, high=len(concated_predicted_classes) - 1, size=20)\n",
    "\n",
    "sample_text = []\n",
    "predicted_class = []\n",
    "\n",
    "for i in rand_idx:\n",
    "    sample_text.append(data_module.predict_data.loc[i, 'text'])\n",
    "    predicted_class.append(model.idx_to_class[concated_predicted_classes[i].item()])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'sample_text': sample_text,\n",
    "    'predicted_class': predicted_class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiinfra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
