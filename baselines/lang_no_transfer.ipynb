{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Example\n",
    "## í˜ì˜¤í‘œí˜„ ë¶„ë¥˜\n",
    "\n",
    "### ì‚¬ìš© ë°©ë²•\n",
    "1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ: https://www.kaggle.com/competitions/kdtai-2/data ì—ì„œ train.csv ì™€ test.csv ë‹¤ìš´ë¡œë“œ\n",
    "2. ë°ì´í„° í´ë”: root/data/Discrimination í´ë”ì— ë‹¤ìš´ë°›ì€ ë‘ csv íŒŒì¼ ìœ„ì¹˜\n",
    "3. Mecab ì´ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´ ê·¸ëƒ¥ ì§„í–‰. ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ Mecab ì„¤ì¹˜ í•­ëª© ì§„í–‰\n",
    "\n",
    "### Mecab ì„¤ì¹˜ ë°©ë²•\n",
    "1. í„°ë¯¸ë„ì—ì„œ root/settings í´ë”ë¡œ ì´ë™\n",
    "2. (ì¤‘ìš”!) ê°€ìƒí™˜ê²½ ì§„ì…\n",
    "3. bash install_mecab.sh ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "def preprocess_korean_text(self, text):\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional)\n",
    "    stop_words = [\"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì—\", \"ì˜\", \"ë¡œ\", \"ìœ¼ë¡œ\", \"ì—ì„œ\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "preprocess_korean_text('ì•ˆë…• ì´ê±´ í…ŒìŠ¤íŠ¸ì•¼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/aiinfra/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Modules About Hydra\n",
    "# from tqdm.notebook import tqdm\n",
    "# from PIL import Image\n",
    "from typing import List, Any\n",
    "# from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS, STEP_OUTPUT\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Modules About Language Pre-processing\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class for korean text\n",
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, word_key_to_index, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.word_key_to_index = word_key_to_index\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.word_key_to_index:\n",
    "                indices.append(self.word_key_to_index[token])\n",
    "            else:\n",
    "                indices.append(self.word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        if not np.isnan(label):\n",
    "            return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "        return torch.tensor(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS\n",
    "\n",
    "\n",
    "class SequentialDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 64, data_dir: str = '../data/') -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.transform = None\n",
    "\n",
    "        self.max_length = 200\n",
    "\n",
    "        # load train data\n",
    "        self.train_data = pd.read_csv(self.data_dir + 'Discrimination/train.csv')\n",
    "\n",
    "        # load predict data\n",
    "        self.predict_data = pd.read_csv(self.data_dir + 'Discrimination/test.csv')\n",
    "\n",
    "        # make word map from train data\n",
    "        print('Making word map')\n",
    "        self._make_word_map(self.train_data)\n",
    "\n",
    "    # custom function for preprocess korean text\n",
    "    def _preprocess_korean_text(self, text):\n",
    "        # Remove URLs and mentions\n",
    "        text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "        text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "        # Tokenize text using Mecab\n",
    "        mecab = Mecab()\n",
    "        tokens = mecab.morphs(text)\n",
    "\n",
    "        # Remove stop words (optional)\n",
    "        stop_words = [\"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì—\", \"ì˜\", \"ë¡œ\", \"ìœ¼ë¡œ\", \"ì—ì„œ\"]\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "        # Remove punctuation and non-Korean characters\n",
    "        tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "        tokens = [t for t in tokens if t]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # custom function for making word map\n",
    "    def _make_word_map(self, train_data):\n",
    "        self.word_index_to_key = []\n",
    "        self.word_key_to_index = {}\n",
    "\n",
    "        for i in range(len(train_data)):\n",
    "            text = train_data.iloc[i]['text']\n",
    "            tokens = self._preprocess_korean_text(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in self.word_key_to_index:\n",
    "                    self.word_key_to_index[token] = len(self.word_index_to_key)\n",
    "                    self.word_index_to_key.append(token)\n",
    "\n",
    "        self.word_key_to_index['<unk>'] = len(self.word_index_to_key)\n",
    "        self.word_index_to_key.append('<unk>')\n",
    "\n",
    "    # just write for downloading actions\n",
    "    def prepare_data(self) -> None:\n",
    "        # make predict dataset\n",
    "        self.predict_dataset = KoreanTextDataset(\n",
    "            data=self.predict_data,\n",
    "            word_key_to_index=self.word_key_to_index,\n",
    "            preprocess_korean_text=self._preprocess_korean_text,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == 'fit':\n",
    "            train_test_ratio = 0.9\n",
    "            train_val_ratio = 0.8\n",
    "\n",
    "            # make train dataset\n",
    "            train_dataset = KoreanTextDataset(\n",
    "                data=self.train_data,\n",
    "                word_key_to_index=self.word_key_to_index,\n",
    "                preprocess_korean_text=self._preprocess_korean_text,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "\n",
    "            # split train val test dataset\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "                train_dataset, [train_test_ratio * train_val_ratio, train_test_ratio * (1 - train_val_ratio), 1 - train_test_ratio]\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom attention class\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom rnn_lstm_attention class\n",
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=LSTM_layers, bidirectional=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        lstm_outputs, (hidden, _) = self.lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        # h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        h = hidden[-1,:,:]\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = self.attention(h, lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(pl.LightningModule):\n",
    "    def __init__(self, word_index_to_key, config=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # define loss function\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # define model\n",
    "        self.model = RNN_LSTM_attention(\n",
    "            vocab_size=len(word_index_to_key),\n",
    "            embedding_dim=100,\n",
    "            hidden_dim=512,\n",
    "            output_dim=7,\n",
    "            LSTM_layers=2,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # etc custom attributes\n",
    "        self.idx_to_class = {\n",
    "            0: 'Origin(ì¶œì‹ ì°¨ë³„)',\n",
    "            1: 'Physical(ì™¸ëª¨ì°¨ë³„) ì™¸ëª¨(ì‹ ì²´, ì–¼êµ´) ë° ì¥ì• ì¸ ì°¨ë³„ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.',\n",
    "            2: 'Politics(ì •ì¹˜ì„±í–¥ì°¨ë³„)',\n",
    "            3: 'Profanity(í˜ì˜¤ìš•ì„¤) ìš•ì„¤,ì €ì£¼,í˜ì˜¤ ë‹¨ì–´, ë¹„ì†ì–´ ë° ê¸°íƒ€ í˜ì˜¤ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.',\n",
    "            4: 'Age(ì—°ë ¹ì°¨ë³„)',\n",
    "            5: 'Gender(ì„±ì°¨ë³„) ì„±ë³„ ë˜ëŠ” ì„±ì  ì·¨í–¥ì— ëŒ€í•œ ì°¨ë³„ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.',\n",
    "            6: 'Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)',\n",
    "        }\n",
    "\n",
    "    def forward(self, x, y=None) -> Any:\n",
    "        output = self.model(x)\n",
    "        if y:\n",
    "            loss = self.loss_func(output, y)\n",
    "            return loss, output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SequentialTask(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # define accuracy function\n",
    "        self.acc_func = Accuracy(\n",
    "            task='multiclass',\n",
    "            num_classes=7\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'train_acc': acc,\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        self.training_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'val_acc': acc,\n",
    "            'val_loss': loss,\n",
    "        }\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not (self.training_step_outputs and self.validation_step_outputs):\n",
    "            return\n",
    "\n",
    "        train_avg_loss = torch.stack([x[\"train_loss\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        train_avg_acc = torch.stack([x[\"train_acc\"]\n",
    "            for x in self.training_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"train_avg_acc\": train_avg_acc,\n",
    "            \"train_avg_loss\": train_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        val_avg_loss = torch.stack([x[\"val_loss\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        val_avg_acc = torch.stack([x[\"val_acc\"]\n",
    "            for x in self.validation_step_outputs]).mean()\n",
    "        metrics = {\n",
    "            \"val_avg_acc\": val_avg_acc,\n",
    "            \"val_avg_loss\": val_avg_loss\n",
    "        }\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "        print(\"\\n\" +\n",
    "              (f'Epoch {self.current_epoch}, Avg. Training Loss: {train_avg_loss:.3f}, Avg. Training Accuracy: {train_avg_acc:.3f} ' +\n",
    "               f'Avg. Validation Loss: {val_avg_loss:.3f}, Avg. Validation Accuracy: {val_avg_acc:.3f}'), flush=True)\n",
    "\n",
    "        self.training_step_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        x, y = batch\n",
    "\n",
    "        loss, output = self.model(x, y)\n",
    "        acc = self.acc_func(output, y)\n",
    "\n",
    "        metrics = {\n",
    "            'test_acc': acc,\n",
    "            'test_loss': loss,\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor:\n",
    "        return torch.argmax(self.model(batch), dim=-1)\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making word map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | SequentialModel    | 7.6 M \n",
      "1 | acc_func | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "3.7 M     Non-trainable params\n",
      "7.6 M     Total params\n",
      "30.249    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [02:51<00:00,  8.65it/s, v_num=12, train_acc=0.633, train_loss=1.020]\n",
      "Epoch 0, Avg. Training Loss: 1.170, Avg. Training Accuracy: 0.605 Avg. Validation Loss: 0.907, Avg. Validation Accuracy: 0.697\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [02:50<00:00,  8.69it/s, v_num=12, train_acc=0.733, train_loss=0.711, val_acc=0.701, val_loss=0.901]\n",
      "Epoch 1, Avg. Training Loss: 0.827, Avg. Training Accuracy: 0.724 Avg. Validation Loss: 0.790, Avg. Validation Accuracy: 0.732\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [02:48<00:00,  8.77it/s, v_num=12, train_acc=0.667, train_loss=0.653, val_acc=0.732, val_loss=0.789]\n",
      "Epoch 2, Avg. Training Loss: 0.697, Avg. Training Accuracy: 0.763 Avg. Validation Loss: 0.731, Avg. Validation Accuracy: 0.752\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [02:48<00:00,  8.80it/s, v_num=12, train_acc=0.733, train_loss=0.485, val_acc=0.752, val_loss=0.731]\n",
      "Epoch 3, Avg. Training Loss: 0.600, Avg. Training Accuracy: 0.794 Avg. Validation Loss: 0.732, Avg. Validation Accuracy: 0.753\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [02:48<00:00,  8.78it/s, v_num=12, train_acc=0.800, train_loss=0.569, val_acc=0.754, val_loss=0.732]\n",
      "Epoch 4, Avg. Training Loss: 0.516, Avg. Training Accuracy: 0.822 Avg. Validation Loss: 0.767, Avg. Validation Accuracy: 0.755\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [02:50<00:00,  8.69it/s, v_num=12, train_acc=0.867, train_loss=0.478, val_acc=0.755, val_loss=0.768]\n",
      "Epoch 5, Avg. Training Loss: 0.447, Avg. Training Accuracy: 0.844 Avg. Validation Loss: 0.793, Avg. Validation Accuracy: 0.759\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [03:11<00:00,  7.72it/s, v_num=12, train_acc=0.867, train_loss=0.478, val_acc=0.759, val_loss=0.793]\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:10<00:00, 19.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.7610082030296326     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     0.785693883895874     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.7610082030296326    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    0.785693883895874    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.7610082030296326, 'test_loss': 0.785693883895874}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = SequentialDataModule(batch_size=32)\n",
    "\n",
    "model = SequentialModel(data_module.word_index_to_key)\n",
    "task = SequentialTask(model)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_acc',\n",
    "    save_top_k=3,\n",
    "    mode='max'\n",
    "))\n",
    "\n",
    "callbacks.append(EarlyStopping(\n",
    "    monitor='val_avg_acc',\n",
    "    min_delta=0.01,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='max',\n",
    "    # stopping_threshold=,\n",
    "    # divergence_threshold=,\n",
    "    # check_finite=,\n",
    "    # check_on_train_epoch_end=,\n",
    "))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.fit(model=task, datamodule=data_module)\n",
    "trainer.test(model=task, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making word map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422/422 [00:23<00:00, 18.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = SequentialDataModule(batch_size=32)\n",
    "\n",
    "model = SequentialModel(data_module.word_index_to_key)\n",
    "task = SequentialTask.load_from_checkpoint('./lightning_logs/version_12/checkpoints/epoch=5-step=8892.ckpt', model=model)\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "predicted_classes = trainer.predict(task, datamodule=data_module)\n",
    "concated_predicted_classes = torch.concat(predicted_classes, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_text</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excelsior!</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"ë³´ìˆ˜ì¢€ë¹„ë“¤ì„. ì•„ì¤Œë§ˆ, í‹€ë”±ë…¸ë•…, ì¼ë² ë‚¨ìë“¤ì´ ë§ìŒ\"</td>\n",
       "      <td>Age(ì—°ë ¹ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"ì•„ì§ë„ 5%ì•¼? ì–´ë–¤ ì¹œì¼íŒŒë†ˆë“¤ì´ ìª½ë°”ë¦¬ ì œí’ˆì— ~~~\"</td>\n",
       "      <td>Origin(ì¶œì‹ ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"ì¢‹ì•„ ì£½ì„ë•ŒëŠ” ì„¹ìŠ¤íŒŒíŠ¸ë„ˆ, í—¤ì–´ì§€ë©´ ì„±ë…¸ë¦¬ê°œ, ì´ê²Œ ë¯¸íˆ¬ëƒ?\"</td>\n",
       "      <td>Gender(ì„±ì°¨ë³„) ì„±ë³„ ë˜ëŠ” ì„±ì  ì·¨í–¥ì— ëŒ€í•œ ì°¨ë³„ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"ìª½ë¹ ë¦¬ë‹¹ í˜„ì •ê¶Œ ì•„ë² ì„¸ë¼ì™€ ì—°í•©í•˜ì—¬ íŒŒê´´ì‹œë„ì¤‘\"</td>\n",
       "      <td>Origin(ì¶œì‹ ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ë°•ê·¼í˜œ ì•„ë‹ˆê²  ì¥</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"ì§±ê¹¨ ì¡°ì„ ì¡± ì¢…ë¶ì£¼ì‚¬íŒŒë“¤ ê³µí†µì  - ë³¸ì¸ë“¤ì„ ë°˜ëŒ€í•˜ë©´ ê¸°ìŠ¹ì „ í† ì™œëª°ì´\"</td>\n",
       "      <td>Origin(ì¶œì‹ ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"ì´ì¯¤ë˜ë©´ ë§‰ ê°€ìëŠ”ê±°ì§€ìš”? ë»˜ê²Œì´ ìƒˆë¼ë“¤ ì¡´ë‚˜ ì³ ë§ì•„ì•¼ ë˜ê² ë„¤~\"</td>\n",
       "      <td>Profanity(í˜ì˜¤ìš•ì„¤) ìš•ì„¤,ì €ì£¼,í˜ì˜¤ ë‹¨ì–´, ë¹„ì†ì–´ ë° ê¸°íƒ€ í˜ì˜¤ ë°œì–¸ì„ í¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ì™€~ ì ë‹¹íˆë“¤ ì¢€ í•´ë¼ 10ë…„ì „ ì¼ì— ì§€ê¸ˆì™€ì„œ í‹°ë¹„ì— ì¢€ ìì£¼ë³´ì¸ë‹¤ê³  ë¯¸íˆ¬??ã…ã…ã…...</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"ë¬¸ì¬ì¸ ë…¸ë¬´í˜„ ë‘ë¶„ë‹¤ ì¸ë³µì´ ì—†ë‹¤..ì €ëŸ° ì“°ë˜ê¸°ë“¤ì„ ë‹¬ê³ ë‹¤ë‹ˆë‹ˆ...\"</td>\n",
       "      <td>Politics(ì •ì¹˜ì„±í–¥ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ìµœê³ ì˜ ì„±í˜•ì€ ë‹¤ì´ì–´íŠ¸ ë§ë„¤ìš”.ëª¸ê´€ë¦¬ ì˜ í•˜ì…”ì„œ ì˜¤ë˜ì˜¤ë˜ ìœ ì§€í•˜ì„¸ìš”.</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ì¬ë¯¸ìˆì„ ê±° ê°™ì€ë° ì™œ ë‹¤ë“¤ ë‚œë¦¬ì¸ì§€.</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"í•˜ì—¬ê°„ ì „ë¼ë„ ìƒˆí‚¤ë“¤ì´ ë¬¸ì œã…‹ã…‹\"</td>\n",
       "      <td>Origin(ì¶œì‹ ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"ì •ë¶€ ì‘ì› ì•ˆí•˜ëŠ” ìª½ë°”ë¦¬ë“¤ì€ ì¼ë³¸ìœ¼ë¡œ ê°€ë¼ ë„ˆí¬ ì•„ë² ì´ê°€ ê¸°ë‹¤ë¦°ë‹¤\"</td>\n",
       "      <td>Origin(ì¶œì‹ ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"ì…ì´ì•¼ ë˜¥êµ¬ë©ì´ì•¼? ã…‹ã…‹\"</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ëŒ€í•™ê°€ ì—°ê·¹ ë™ì•„ë¦¬ë“¤ë„ í„¸ì–´ë´ë¼ ê±°ê¸° ì—„ì²­ ë¬¼ë€í•˜ê³  ì‹¬ê°í•˜ë”ë¼</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"ëª»ìƒê¸´ê±° ë³´ê¸°ì‹«ë‹¤. í† ë‚˜ì™€. ì˜ìƒê¸´ ì¡°êµ­ ì¥ê´€ë‹˜ë§Œ ë³¼ë˜ìš©ğŸ’™ğŸ’™ğŸ’™ğŸ’™ğŸ’™\"</td>\n",
       "      <td>Physical(ì™¸ëª¨ì°¨ë³„) ì™¸ëª¨(ì‹ ì²´, ì–¼êµ´) ë° ì¥ì• ì¸ ì°¨ë³„ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ì—­ì‹œ 1í™”ë¶€í„° ì¡´ë‚˜ ì¬ë¯¸ì—†ì—ˆìŒ ã…‹ã…‹ã…‹ã…‹</td>\n",
       "      <td>Profanity(í˜ì˜¤ìš•ì„¤) ìš•ì„¤,ì €ì£¼,í˜ì˜¤ ë‹¨ì–´, ë¹„ì†ì–´ ë° ê¸°íƒ€ í˜ì˜¤ ë°œì–¸ì„ í¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"ì €ê²ƒì´ ë¼ë„ ì¦ë¼ë„ì™€ ë¯¼ì£¼ë‹¹ì€ ë¨¸ë¦¬ ì¡°ì•„ë¦¬ê³  ë”¸ë‘ë”¸ë‘ í•˜ê²ƒì§€? ã…\"</td>\n",
       "      <td>Origin(ì¶œì‹ ì°¨ë³„)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ë¯¸êµ­ì‚° ì‡ ê³ ê¸°ë¨¹ìœ¼ë©´ ë’¤ì§„ë‹¤ê³  ì‚¬ê¸°ì¹œë„˜ë“¤ì´ë‹¤ ë­”ë“¤ ëª»í•˜ê² ëƒ</td>\n",
       "      <td>Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sample_text  \\\n",
       "0                                          Excelsior!   \n",
       "1                      \"ë³´ìˆ˜ì¢€ë¹„ë“¤ì„. ì•„ì¤Œë§ˆ, í‹€ë”±ë…¸ë•…, ì¼ë² ë‚¨ìë“¤ì´ ë§ìŒ\"   \n",
       "2                    \"ì•„ì§ë„ 5%ì•¼? ì–´ë–¤ ì¹œì¼íŒŒë†ˆë“¤ì´ ìª½ë°”ë¦¬ ì œí’ˆì— ~~~\"   \n",
       "3                 \"ì¢‹ì•„ ì£½ì„ë•ŒëŠ” ì„¹ìŠ¤íŒŒíŠ¸ë„ˆ, í—¤ì–´ì§€ë©´ ì„±ë…¸ë¦¬ê°œ, ì´ê²Œ ë¯¸íˆ¬ëƒ?\"   \n",
       "4                         \"ìª½ë¹ ë¦¬ë‹¹ í˜„ì •ê¶Œ ì•„ë² ì„¸ë¼ì™€ ì—°í•©í•˜ì—¬ íŒŒê´´ì‹œë„ì¤‘\"   \n",
       "5                                           ë°•ê·¼í˜œ ì•„ë‹ˆê²  ì¥   \n",
       "6            \"ì§±ê¹¨ ì¡°ì„ ì¡± ì¢…ë¶ì£¼ì‚¬íŒŒë“¤ ê³µí†µì  - ë³¸ì¸ë“¤ì„ ë°˜ëŒ€í•˜ë©´ ê¸°ìŠ¹ì „ í† ì™œëª°ì´\"   \n",
       "7              \"ì´ì¯¤ë˜ë©´ ë§‰ ê°€ìëŠ”ê±°ì§€ìš”? ë»˜ê²Œì´ ìƒˆë¼ë“¤ ì¡´ë‚˜ ì³ ë§ì•„ì•¼ ë˜ê² ë„¤~\"   \n",
       "8   ì™€~ ì ë‹¹íˆë“¤ ì¢€ í•´ë¼ 10ë…„ì „ ì¼ì— ì§€ê¸ˆì™€ì„œ í‹°ë¹„ì— ì¢€ ìì£¼ë³´ì¸ë‹¤ê³  ë¯¸íˆ¬??ã…ã…ã…...   \n",
       "9             \"ë¬¸ì¬ì¸ ë…¸ë¬´í˜„ ë‘ë¶„ë‹¤ ì¸ë³µì´ ì—†ë‹¤..ì €ëŸ° ì“°ë˜ê¸°ë“¤ì„ ë‹¬ê³ ë‹¤ë‹ˆë‹ˆ...\"   \n",
       "10             ìµœê³ ì˜ ì„±í˜•ì€ ë‹¤ì´ì–´íŠ¸ ë§ë„¤ìš”.ëª¸ê´€ë¦¬ ì˜ í•˜ì…”ì„œ ì˜¤ë˜ì˜¤ë˜ ìœ ì§€í•˜ì„¸ìš”.   \n",
       "11                              ì¬ë¯¸ìˆì„ ê±° ê°™ì€ë° ì™œ ë‹¤ë“¤ ë‚œë¦¬ì¸ì§€.   \n",
       "12                                \"í•˜ì—¬ê°„ ì „ë¼ë„ ìƒˆí‚¤ë“¤ì´ ë¬¸ì œã…‹ã…‹\"   \n",
       "13             \"ì •ë¶€ ì‘ì› ì•ˆí•˜ëŠ” ìª½ë°”ë¦¬ë“¤ì€ ì¼ë³¸ìœ¼ë¡œ ê°€ë¼ ë„ˆí¬ ì•„ë² ì´ê°€ ê¸°ë‹¤ë¦°ë‹¤\"   \n",
       "14                                    \"ì…ì´ì•¼ ë˜¥êµ¬ë©ì´ì•¼? ã…‹ã…‹\"   \n",
       "15                 ëŒ€í•™ê°€ ì—°ê·¹ ë™ì•„ë¦¬ë“¤ë„ í„¸ì–´ë´ë¼ ê±°ê¸° ì—„ì²­ ë¬¼ë€í•˜ê³  ì‹¬ê°í•˜ë”ë¼   \n",
       "16             \"ëª»ìƒê¸´ê±° ë³´ê¸°ì‹«ë‹¤. í† ë‚˜ì™€. ì˜ìƒê¸´ ì¡°êµ­ ì¥ê´€ë‹˜ë§Œ ë³¼ë˜ìš©ğŸ’™ğŸ’™ğŸ’™ğŸ’™ğŸ’™\"   \n",
       "17                              ì—­ì‹œ 1í™”ë¶€í„° ì¡´ë‚˜ ì¬ë¯¸ì—†ì—ˆìŒ ã…‹ã…‹ã…‹ã…‹   \n",
       "18             \"ì €ê²ƒì´ ë¼ë„ ì¦ë¼ë„ì™€ ë¯¼ì£¼ë‹¹ì€ ë¨¸ë¦¬ ì¡°ì•„ë¦¬ê³  ë”¸ë‘ë”¸ë‘ í•˜ê²ƒì§€? ã…\"   \n",
       "19                    ë¯¸êµ­ì‚° ì‡ ê³ ê¸°ë¨¹ìœ¼ë©´ ë’¤ì§„ë‹¤ê³  ì‚¬ê¸°ì¹œë„˜ë“¤ì´ë‹¤ ë­”ë“¤ ëª»í•˜ê² ëƒ   \n",
       "\n",
       "                                      predicted_class  \n",
       "0                             Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "1                                           Age(ì—°ë ¹ì°¨ë³„)  \n",
       "2                                        Origin(ì¶œì‹ ì°¨ë³„)  \n",
       "3           Gender(ì„±ì°¨ë³„) ì„±ë³„ ë˜ëŠ” ì„±ì  ì·¨í–¥ì— ëŒ€í•œ ì°¨ë³„ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.  \n",
       "4                                        Origin(ì¶œì‹ ì°¨ë³„)  \n",
       "5                             Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "6                                        Origin(ì¶œì‹ ì°¨ë³„)  \n",
       "7   Profanity(í˜ì˜¤ìš•ì„¤) ìš•ì„¤,ì €ì£¼,í˜ì˜¤ ë‹¨ì–´, ë¹„ì†ì–´ ë° ê¸°íƒ€ í˜ì˜¤ ë°œì–¸ì„ í¬...  \n",
       "8                             Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "9                                    Politics(ì •ì¹˜ì„±í–¥ì°¨ë³„)  \n",
       "10                            Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "11                            Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "12                                       Origin(ì¶œì‹ ì°¨ë³„)  \n",
       "13                                       Origin(ì¶œì‹ ì°¨ë³„)  \n",
       "14                            Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "15                            Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  \n",
       "16      Physical(ì™¸ëª¨ì°¨ë³„) ì™¸ëª¨(ì‹ ì²´, ì–¼êµ´) ë° ì¥ì• ì¸ ì°¨ë³„ ë°œì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.  \n",
       "17  Profanity(í˜ì˜¤ìš•ì„¤) ìš•ì„¤,ì €ì£¼,í˜ì˜¤ ë‹¨ì–´, ë¹„ì†ì–´ ë° ê¸°íƒ€ í˜ì˜¤ ë°œì–¸ì„ í¬...  \n",
       "18                                       Origin(ì¶œì‹ ì°¨ë³„)  \n",
       "19                            Not Hate Speech(í•´ë‹¹ì‚¬í•­ì—†ìŒ)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx = np.random.randint(low=0, high=len(concated_predicted_classes) - 1, size=20)\n",
    "\n",
    "sample_text = []\n",
    "predicted_class = []\n",
    "\n",
    "for i in rand_idx:\n",
    "    sample_text.append(data_module.predict_data.loc[i, 'text'])\n",
    "    predicted_class.append(model.idx_to_class[concated_predicted_classes[i].item()])\n",
    "\n",
    "pd.DataFrame({\n",
    "    'sample_text': sample_text,\n",
    "    'predicted_class': predicted_class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiinfra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
