{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "필요한 모듈을 Import 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Modules About Hydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Modules About Torch, Numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Modules About Pytorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, LightningDataModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ProgressBar\n",
    "\n",
    "# Modules About Pandas, Matplotlib, Numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Others\n",
    "from PIL import Image\n",
    "from typing import List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Dataset\n",
    "\n",
    "Custom Dataset을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, pred_batch_size, train_ratio, pred_dataset=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.pred_batch_size = pred_batch_size\n",
    "        self.train_ratio = train_ratio\n",
    "        self.pred_dataset = pred_dataset\n",
    "\n",
    "        # Define Transforms\n",
    "        def repeat_channels(x):\n",
    "            return x.repeat(3, 1, 1)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            # ViT expects 224x224 images\n",
    "            # transforms.Resize((224, 224), antialias=True),\n",
    "            # transforms.Lambda(repeat_channels)  # ViT expects 3 channels\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download MNIST Data\n",
    "        datasets.MNIST(\n",
    "            self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(\n",
    "            self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        mnist_train = datasets.MNIST(\n",
    "            self.data_dir, train=True, transform=self.transform)\n",
    "        mnist_test = datasets.MNIST(\n",
    "            self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        # Split Dataset\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            mnist_train, list(map(lambda x: int(x * len(mnist_train)), [self.train_ratio, 1-self.train_ratio])))\n",
    "        self.test_dataset = mnist_test\n",
    "\n",
    "    # def _collate_fn(self, samples):\n",
    "    #     이 함수를 사용할 경우\n",
    "    #     DataLoader에 인자로 collate_fn=_collate_fn 를 추가해야합니다.\n",
    "    #     pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.pred_dataset, batch_size=self.pred_batch_size)\n",
    "\n",
    "    def predict_instantly(self, x: List[Any], y: List[int]):\n",
    "        to_tensor = torchvision.transforms.ToTensor()\n",
    "        tensor_x = torch.stack([to_tensor(item) for item in x])\n",
    "        tensor_y = torch.tensor(y)\n",
    "\n",
    "        return tensor_x, tensor_y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Model\n",
    "\n",
    "Model 구조를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(self, type):\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "        self.save_hyperparameters(\"type\")\n",
    "\n",
    "        self.model_list = {\"small\": (32, 64), \"large\": (64, 128)}\n",
    "        self.id2label = {i: i for i in range(10)}\n",
    "        self.label2id = {i: i for i in range(10)}\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.model = nn.Sequential(\n",
    "            # Convolutional layer 1\n",
    "            nn.Conv2d(1, self.model_list[self.type][0],\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional layer 2\n",
    "            nn.Conv2d(self.model_list[self.type][0], self.model_list[self.type]\n",
    "                      [1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Fully connected layers\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.model_list[self.type][1] * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 10),  # assuming output has 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        logits = self.model(x)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        return loss, logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Model\n",
    "\n",
    "Task 구조를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model, num_classes, optimizer, lr_scheduler=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.save_hyperparameters(\"num_classes\", \"optimizer\", \"lr_scheduler\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        loss, logits = self.model(x, y)\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_step(batch)\n",
    "        metrics = {\"train_acc\": acc, \"train_loss\": loss}\n",
    "        self.training_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # def on_train_epoch_end(self):\n",
    "    #     pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_step(batch)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.validation_step_outputs.append(metrics)\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        if self.training_step_outputs:\n",
    "            train_avg_loss = torch.stack([x[\"train_loss\"]\n",
    "                                          for x in self.training_step_outputs]).mean()\n",
    "            train_avg_acc = torch.stack([x[\"train_acc\"]\n",
    "                                        for x in self.training_step_outputs]).mean()\n",
    "            metrics = {\"train_avg_acc\": train_avg_acc,\n",
    "                       \"train_avg_loss\": train_avg_loss}\n",
    "            self.log_dict(metrics)\n",
    "        else:\n",
    "            return\n",
    "        if self.validation_step_outputs:\n",
    "            val_avg_loss = torch.stack([x[\"val_loss\"]\n",
    "                                        for x in self.validation_step_outputs]).mean()\n",
    "            val_avg_acc = torch.stack([x[\"val_acc\"]\n",
    "                                       for x in self.validation_step_outputs]).mean()\n",
    "            metrics = {\"val_avg_acc\": val_avg_acc,\n",
    "                       \"val_avg_loss\": val_avg_loss}\n",
    "            self.log_dict(metrics)\n",
    "        else:\n",
    "            return\n",
    "        print(\"\\n\" +\n",
    "              (f'Epoch {self.current_epoch}, Avg. Training Loss: {train_avg_loss:.3f}, Avg. Training Accuracy: {train_avg_acc:.3f} ' +\n",
    "               f'Avg. Validation Loss: {val_avg_loss:.3f}, Avg. Validation Accuracy: {val_avg_acc:.3f}'), flush=True)\n",
    "        self.training_step_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_step(batch)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        x, y = batch\n",
    "        loss, logits = self.model(x, y)\n",
    "        acc_fn = torchmetrics.classification.MulticlassAccuracy(\n",
    "            num_classes=self.num_classes).to(self.device)\n",
    "        acc = acc_fn(logits, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        loss, logits = self.model(x, y)\n",
    "        return loss, logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer\n",
    "        if self.lr_scheduler is not None:\n",
    "            return [optimizer], [self.lr_scheduler]\n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "        # return torch.optim.AdamW(self.model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 2e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"./\"):\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "models = [instantiate(cfg.models[model]) for model in dir(cfg.models)]\n",
    "optimizer = instantiate(cfg.task.optimizer, params=models[0].parameters())\n",
    "optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Model Training을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | CNNModel | 429 K \n",
      "-----------------------------------\n",
      "429 K     Trainable params\n",
      "0         Non-trainable params\n",
      "429 K     Total params\n",
      "1.717     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=choice(32, 128, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m task \u001b[39m=\u001b[39m ClassificationTask(\n\u001b[1;32m     49\u001b[0m     model\u001b[39m=\u001b[39mmodel, num_classes\u001b[39m=\u001b[39mnum_classes, optimizer\u001b[39m=\u001b[39moptimizer, lr_scheduler\u001b[39m=\u001b[39mlr_scheduler)\n\u001b[1;32m     50\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mmax_epochs,\n\u001b[1;32m     51\u001b[0m                      callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m---> 53\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(task, data_module)\n\u001b[1;32m     54\u001b[0m trainer\u001b[39m.\u001b[39mtest(task, datamodule\u001b[39m=\u001b[39mdata_module)\n\u001b[1;32m     55\u001b[0m trainer\u001b[39m.\u001b[39msave_checkpoint(\u001b[39m\"\u001b[39m\u001b[39mtest_checkpoints/best_model.ckpt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    529\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 531\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    533\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    561\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    562\u001b[0m )\n\u001b[1;32m    564\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    565\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    566\u001b[0m     ckpt_path,\n\u001b[1;32m    567\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    568\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m )\n\u001b[0;32m--> 570\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    572\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1015\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1016\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1017\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1018\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1045\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1044\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1047\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:98\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@_no_grad_context\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[_OUT_DICT]:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_data()\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip:\n\u001b[1;32m    100\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:150\u001b[0m, in \u001b[0;36m_EvaluationLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m stage \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_source\n\u001b[0;32m--> 150\u001b[0m dataloaders \u001b[39m=\u001b[39m _request_dataloader(source)\n\u001b[1;32m    151\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m.\u001b[39mdataloader_prefix\u001b[39m}\u001b[39;00m\u001b[39m_dataloader()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloaders, CombinedLoader):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:330\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[1;32m    322\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39m    The requested dataloader\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    326\u001b[0m     \u001b[39m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[39m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[39m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[39m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     \u001b[39mreturn\u001b[39;00m data_source\u001b[39m.\u001b[39;49mdataloader()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:300\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    299\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\u001b[39m.\u001b[39mtrainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_lightning_datamodule_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance\u001b[39m.\u001b[39;49mtrainer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m    301\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:160\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    159\u001b[0m     \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningDataModule]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mMNISTDataModule.val_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mval_dataloader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_dataset, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/lightning_fabric/utilities/data.py:326\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    324\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[0;32m--> 326\u001b[0m init(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    327\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/torch/utils/data/dataloader.py:357\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    353\u001b[0m             sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m batch_sampler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     batch_sampler \u001b[39m=\u001b[39m BatchSampler(sampler, batch_size, drop_last)\n\u001b[1;32m    359\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_last \u001b[39m=\u001b[39m drop_last\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/lightning_fabric/utilities/data.py:326\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    324\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[0;32m--> 326\u001b[0m init(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    327\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/infra/lib/python3.10/site-packages/torch/utils/data/sampler.py:232\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, sampler: Union[Sampler[\u001b[39mint\u001b[39m], Iterable[\u001b[39mint\u001b[39m]], batch_size: \u001b[39mint\u001b[39m, drop_last: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m# check here.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mbool\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    231\u001b[0m             batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbatch_size should be a positive integer value, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mbut got batch_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(batch_size))\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(drop_last, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdrop_last should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mdrop_last=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(drop_last))\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=choice(32, 128, 32)"
     ]
    }
   ],
   "source": [
    "# Load Configuration Object\n",
    "with initialize(version_base=None, config_path=\"./\"):\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "\n",
    "# Load Data Module\n",
    "data_module = MNISTDataModule(data_dir=cfg.data.data_dir,\n",
    "                              batch_size=cfg.data.batch_size,\n",
    "                              pred_batch_size=cfg.data.pred_batch_size,\n",
    "                              train_ratio=cfg.data.train_ratio)\n",
    "\n",
    "# Load Training Configuration\n",
    "models = [instantiate(cfg.models[model]) for model in dir(cfg.models)]\n",
    "\n",
    "# Add Callbacks\n",
    "cfg_callbacks = cfg.train.callbacks\n",
    "callbacks = []\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=cfg_callbacks.checkpoint_callback.monitor,  # 모니터링 할 메트릭을 지정\n",
    "    dirpath=cfg_callbacks.checkpoint_callback.dirpath,  # checkpoint 저장 경로\n",
    "    filename=cfg_callbacks.checkpoint_callback.filename,  # 저장될 파일 이름\n",
    "    # 가장 좋은 k개의 모델만 저장 (1로 설정하면 최고의 하나만 저장)\n",
    "    save_top_k=cfg_callbacks.checkpoint_callback.save_top_k,\n",
    "    mode=cfg_callbacks.checkpoint_callback.mode,  # 'val_acc'를 최소화 하기 위해 'max'를 설정\n",
    ")\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=cfg_callbacks.early_stop_callback.monitor,\n",
    "    min_delta=cfg_callbacks.early_stop_callback.min_delta,\n",
    "    patience=cfg_callbacks.early_stop_callback.patience,\n",
    "    verbose=cfg_callbacks.early_stop_callback.verbose,\n",
    "    mode=cfg_callbacks.early_stop_callback.mode\n",
    ")\n",
    "callbacks.append(early_stop_callback)\n",
    "\n",
    "num_classes = cfg.task.num_classes\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    optimizer = instantiate(cfg.task.optimizer, params=model.parameters())\n",
    "    if cfg.task.lr_scheduler.scheduler._target_ is not None:\n",
    "        lr_scheduler = {}\n",
    "        lr_scheduler[\"scheduler\"] = instantiate(\n",
    "            cfg.task.lr_scheduler.scheduler, optimizer=optimizer)\n",
    "        lr_scheduler[\"interval\"] = cfg.task.lr_scheduler.interval\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "    task = ClassificationTask(\n",
    "        model=model, num_classes=num_classes, optimizer=optimizer, lr_scheduler=lr_scheduler)\n",
    "    trainer = pl.Trainer(max_epochs=cfg.train.max_epochs,\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "    trainer.fit(task, data_module)\n",
    "    trainer.test(task, datamodule=data_module)\n",
    "    trainer.save_checkpoint(\"test_checkpoints/best_model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | CNNModel | 429 K \n",
      "-----------------------------------\n",
      "429 K     Trainable params\n",
      "0         Non-trainable params\n",
      "429 K     Total params\n",
      "1.717     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  66%|██████▋   | 571/860 [00:30<00:15, 18.85it/s, v_num=126, train_acc=0.943, train_loss=0.274, val_acc=0.885, val_loss=0.366]\n",
      "Epoch 0: 100%|██████████| 860/860 [00:33<00:00, 25.45it/s, v_num=127, train_acc=0.950, train_loss=0.269, val_acc=0.907, val_loss=0.314]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.022 >= min_delta = 0.003. New best score: 0.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:33<00:00, 25.69it/s, v_num=127, train_acc=0.950, train_loss=0.269, val_acc=0.914, val_loss=0.285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.007 >= min_delta = 0.003. New best score: 0.914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 860/860 [00:33<00:00, 25.51it/s, v_num=127, train_acc=0.950, train_loss=0.262, val_acc=0.922, val_loss=0.259] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.008 >= min_delta = 0.003. New best score: 0.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 860/860 [00:33<00:00, 25.74it/s, v_num=127, train_acc=0.917, train_loss=0.255, val_acc=0.930, val_loss=0.236] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.007 >= min_delta = 0.003. New best score: 0.930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 860/860 [00:33<00:00, 25.71it/s, v_num=127, train_acc=0.917, train_loss=0.244, val_acc=0.937, val_loss=0.216] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.007 >= min_delta = 0.003. New best score: 0.937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 860/860 [00:33<00:00, 25.72it/s, v_num=127, train_acc=0.917, train_loss=0.230, val_acc=0.943, val_loss=0.198] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.006 >= min_delta = 0.003. New best score: 0.943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 860/860 [00:33<00:00, 25.66it/s, v_num=127, train_acc=0.917, train_loss=0.216, val_acc=0.946, val_loss=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.003 >= min_delta = 0.003. New best score: 0.946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 860/860 [00:33<00:00, 25.59it/s, v_num=127, train_acc=0.917, train_loss=0.197, val_acc=0.952, val_loss=0.167] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.005 >= min_delta = 0.003. New best score: 0.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 860/860 [00:33<00:00, 25.74it/s, v_num=127, train_acc=0.917, train_loss=0.179, val_acc=0.956, val_loss=0.155] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.004 >= min_delta = 0.003. New best score: 0.956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 860/860 [00:33<00:00, 25.75it/s, v_num=127, train_acc=0.917, train_loss=0.140, val_acc=0.960, val_loss=0.135] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.005 >= min_delta = 0.003. New best score: 0.960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 860/860 [00:33<00:00, 25.79it/s, v_num=127, train_acc=0.917, train_loss=0.109, val_acc=0.964, val_loss=0.120] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.004 >= min_delta = 0.003. New best score: 0.964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 860/860 [00:33<00:00, 25.76it/s, v_num=127, train_acc=0.950, train_loss=0.0763, val_acc=0.969, val_loss=0.104] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.004 >= min_delta = 0.003. New best score: 0.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 860/860 [00:34<00:00, 24.68it/s, v_num=127, train_acc=1.000, train_loss=0.0583, val_acc=0.970, val_loss=0.0942] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_acc did not improve in the last 3 records. Best score: 0.969. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 860/860 [00:34<00:00, 24.66it/s, v_num=127, train_acc=1.000, train_loss=0.0583, val_acc=0.970, val_loss=0.0942]\n"
     ]
    }
   ],
   "source": [
    "# # 만일 이전 결과에서 Epoch을 이어서 실행하고 싶다면?\n",
    "# model = CNNModel(config=config)  # 기존에 학습 때 사용한 모델\n",
    "# task = ClassificationTask.load_from_checkpoint(\n",
    "#     \"test_checkpoints/checkpoints/last.ckpt\", model=model)  # 기존 최신 모델에서 체크포인트를 가져옴\n",
    "# trainer = pl.Trainer(max_epochs=30, callbacks=callbacks)\n",
    "# trainer.fit(task, data_module)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "\n",
    "직접 Model Prediction을 수행하여 모델이 제대로 동작하는지 검증합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAb50lEQVR4nO3df2zV9b3H8VcL9FihPbXU/jijYAERIz/cULoGRRwd0G1OlGT+4A8wTpQdzKA6TY2CqFm9uKhxYZglBmYUdCQCk2RsWm2JrsVQJYyoHW26AesPlKTnQJFC2s/9g3jmkQJ+D+f03XN4PpJvQs857573vvfI8x56+JLmnHMCAGCApVsvAAC4OBEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgYqj1At/W19entrY2ZWVlKS0tzXodAIBHzjkdPXpUgUBA6elnf58z6ALU1tam4uJi6zUAABfo4MGDGjVq1FnvH3R/BJeVlWW9AgAgDs73+3nCArR27VpdccUVuuSSS1RaWqqPPvroO83xx24AkBrO9/t5QgL05ptvqrKyUqtWrdLHH3+sqVOnau7cuTp8+HAing4AkIxcAkyfPt0Fg8HI1729vS4QCLjq6urzzoZCISeJg4ODgyPJj1AodM7f7+P+DujkyZNqbGxUeXl55Lb09HSVl5ervr7+jMf39PQoHA5HHQCA1Bf3AH355Zfq7e1VQUFB1O0FBQXq6Og44/HV1dXy+/2Rg0/AAcDFwfxTcFVVVQqFQpHj4MGD1isBAAZA3P8eUF5enoYMGaLOzs6o2zs7O1VYWHjG430+n3w+X7zXAAAMcnF/B5SRkaFp06appqYmcltfX59qampUVlYW76cDACSphFwJobKyUosWLdJ1112n6dOn68UXX1R3d7fuueeeRDwdACAJJSRAd9xxh7744gutXLlSHR0duvbaa7Vjx44zPpgAALh4pTnnnPUS3xQOh+X3+63XAABcoFAopOzs7LPeb/4pOADAxYkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJiIe4CefPJJpaWlRR0TJ06M99MAAJLc0ER802uuuUbvvvvu/55kaEKeBgCQxBJShqFDh6qwsDAR3xoAkCIS8jOg/fv3KxAIaOzYsVq4cKEOHDhw1sf29PQoHA5HHQCA1Bf3AJWWlmrDhg3asWOH1q1bp9bWVt144406evRov4+vrq6W3++PHMXFxfFeCQAwCKU551win6Crq0tjxozR888/r3vvvfeM+3t6etTT0xP5OhwOEyEASAGhUEjZ2dlnvT/hnw7IycnRhAkT1Nzc3O/9Pp9PPp8v0WsAAAaZhP89oGPHjqmlpUVFRUWJfioAQBKJe4Aefvhh1dXV6d///rf+8Y9/6LbbbtOQIUN01113xfupAABJLO5/BHfo0CHdddddOnLkiC6//HLdcMMNamho0OWXXx7vpwIAJLGEfwjBq3A4LL/fb70GkFAjRozwPFNVVeV5Zvjw4Z5nJOnBBx+MaW4gvPbaa55nnnrqqZie67///a/nmRMnTsT0XKnofB9C4FpwAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJLkaKlJSZmRnT3KOPPup55rLLLvM8s2TJEs8zGRkZnmcG2X/eSeezzz7zPDNjxgzPM+Fw2PNMMuBipACAQYkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmhlovAJzPTTfd5Hlm5cqVA/Zcg9kXX3wR01woFIrzJv178803Pc8MHz7c88zy5cs9z0jS1Vdf7XkmEAh4nknVq2GfD++AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXIwUA2rWrFmeZ2K5YOXIkSM9zwykzs5OzzNvvPGG55l169Z5npGk5ubmmOYGwoQJEzzP3H333TE9V35+vueZ+++/3/PMihUrPM+kAt4BAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBgplJmZGdPcM88843lm4cKFnmcG8sKisVwkdNOmTZ5n/vjHP3qe+de//uV5ZrCrqKjwPPPkk096nonloqJSbK+H119/PabnuhjxDggAYIIAAQBMeA7Qzp07dcsttygQCCgtLU1bt26Nut85p5UrV6qoqEiZmZkqLy/X/v3747UvACBFeA5Qd3e3pk6dqrVr1/Z7/5o1a/TSSy/p5Zdf1q5duzR8+HDNnTtXJ06cuOBlAQCpw/OHECoqKs76g0PnnF588UU9/vjjuvXWWyVJr776qgoKCrR161bdeeedF7YtACBlxPVnQK2trero6FB5eXnkNr/fr9LSUtXX1/c709PTo3A4HHUAAFJfXAPU0dEhSSooKIi6vaCgIHLft1VXV8vv90eO4uLieK4EABikzD8FV1VVpVAoFDkOHjxovRIAYADENUCFhYWSzvzLW52dnZH7vs3n8yk7OzvqAACkvrgGqKSkRIWFhaqpqYncFg6HtWvXLpWVlcXzqQAASc7zp+COHTum5ubmyNetra3as2ePcnNzNXr0aC1fvlzPPPOMrrzySpWUlOiJJ55QIBDQ/Pnz47k3ACDJeQ7Q7t27dfPNN0e+rqyslCQtWrRIGzZs0COPPKLu7m4tWbJEXV1duuGGG7Rjxw5dcskl8dsaAJD00pxzznqJbwqHw/L7/dZrXFTGjx8f09znn38e503sffOvEHxXtbW18V8kCU2YMMHzzIcffuh55rLLLvM8E6v777/f88wrr7ySgE2SUygUOufP9c0/BQcAuDgRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhOd/jgGD27XXXut55pFHHon/InHU3t7ueSbWKxLX19fHNJdqrrvuOs8zjz32mOeZgbqy9aFDh2KaGzZsWJw3wTfxDggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHFSFPMPffc43nmF7/4RQI2iZ+lS5d6ntm+fXsCNkk+5eXlMc1t3rzZ80xWVlZMz+VVLBcW/fnPfx7Tc+3duzemOXw3vAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMVIMqBdeeMHzzN///vcEbJJ8qqqqBmRGki699NKY5rxqa2vzPPOzn/3M88y+ffs8zyDxeAcEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgYqSD2IgRIzzPLFmyJAGbxM/OnTs9z5w8eTIBm/Rv6dKlnmdycnI8zzz00EOeZ2J5PQwdOrj/Ew8Gg55nuLBo6uAdEADABAECAJjwHKCdO3fqlltuUSAQUFpamrZu3Rp1/+LFi5WWlhZ1zJs3L177AgBShOcAdXd3a+rUqVq7du1ZHzNv3jy1t7dHjk2bNl3QkgCA1OP5J5QVFRWqqKg452N8Pp8KCwtjXgoAkPoS8jOg2tpa5efn66qrrtLSpUt15MiRsz62p6dH4XA46gAApL64B2jevHl69dVXVVNTo//7v/9TXV2dKioq1Nvb2+/jq6ur5ff7I0dxcXG8VwIADEJx/0sCd955Z+TXkydP1pQpUzRu3DjV1tZq9uzZZzy+qqpKlZWVka/D4TARAoCLQMI/hj127Fjl5eWpubm53/t9Pp+ys7OjDgBA6kt4gA4dOqQjR46oqKgo0U8FAEginv8I7tixY1HvZlpbW7Vnzx7l5uYqNzdXq1ev1oIFC1RYWKiWlhY98sgjGj9+vObOnRvXxQEAyc1zgHbv3q2bb7458vXXP79ZtGiR1q1bp7179+pPf/qTurq6FAgENGfOHD399NPy+Xzx2xoAkPQ8B2jWrFlyzp31/r/97W8XtBD+J5aLcDY0NHiemTlzpueZWN14442eZw4cOOB5ZseOHZ5nJCk/Pz+muYGQnu79T8z7+voSsEn/2traPM98+umnCdgEyYJrwQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEmjvXpa0NhMNh+f1+6zWS1vjx4z3PfP755wnYBPGWlpbmeSbW/7x7e3s9z/z2t7/1PLN69WrPM0geoVDonP/KNe+AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATQ60XQHyNGTPG80w4HI7puc51kUEkt8bGRs8zXFgUXvEOCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIU0xNTY3nmTlz5sT0XFdccYXnmeLiYs8zCxcu9DzzyiuveJ6RpN7eXs8zzz33nOeZ4cOHe56JxalTp2Kae/bZZ+O8CXAm3gEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACbSnHPOeolvCofD8vv91mvgIvXXv/7V88yPf/zjBGxypr6+Ps8zjz32WEzP9bvf/S6mOeCbQqGQsrOzz3o/74AAACYIEADAhKcAVVdX6/rrr1dWVpby8/M1f/58NTU1RT3mxIkTCgaDGjlypEaMGKEFCxaos7MzrksDAJKfpwDV1dUpGAyqoaFB77zzjk6dOqU5c+aou7s78pgVK1bo7bff1ubNm1VXV6e2tjbdfvvtcV8cAJDcPP2LqDt27Ij6esOGDcrPz1djY6NmzpypUCikV155RRs3btSPfvQjSdL69et19dVXq6GhQT/84Q/jtzkAIKld0M+AQqGQJCk3N1eS1NjYqFOnTqm8vDzymIkTJ2r06NGqr6/v93v09PQoHA5HHQCA1BdzgPr6+rR8+XLNmDFDkyZNkiR1dHQoIyNDOTk5UY8tKChQR0dHv9+nurpafr8/chQXF8e6EgAgicQcoGAwqH379umNN964oAWqqqoUCoUix8GDBy/o+wEAkoOnnwF9bdmyZdq+fbt27typUaNGRW4vLCzUyZMn1dXVFfUuqLOzU4WFhf1+L5/PJ5/PF8saAIAk5ukdkHNOy5Yt05YtW/Tee++ppKQk6v5p06Zp2LBhqqmpidzW1NSkAwcOqKysLD4bAwBSgqd3QMFgUBs3btS2bduUlZUV+bmO3+9XZmam/H6/7r33XlVWVio3N1fZ2dl68MEHVVZWxifgAABRPAVo3bp1kqRZs2ZF3b5+/XotXrxYkvTCCy8oPT1dCxYsUE9Pj+bOnas//OEPcVkWAJA6uBgpUtLNN98c09xf/vIXzzOZmZkxPZdX//znPz3PfP/730/AJsB3w8VIAQCDEgECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEzE9C+iAgOpqKjI88xPf/rTmJ5roK5sHYunnnrKegUgrngHBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GKkGPSuvfZazzPLly+P+x7xtG7dOs8zW7ZsScAmgB3eAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgYKQa9srIy6xXi7umnn7ZeATDHOyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQXI8Wg99prr3me+eUvfxnTc+Xn53ueeemllzzPdHV1eZ4BUg3vgAAAJggQAMCEpwBVV1fr+uuvV1ZWlvLz8zV//nw1NTVFPWbWrFlKS0uLOh544IG4Lg0ASH6eAlRXV6dgMKiGhga98847OnXqlObMmaPu7u6ox913331qb2+PHGvWrInr0gCA5OfpQwg7duyI+nrDhg3Kz89XY2OjZs6cGbn90ksvVWFhYXw2BACkpAv6GVAoFJIk5ebmRt3++uuvKy8vT5MmTVJVVZWOHz9+1u/R09OjcDgcdQAAUl/MH8Pu6+vT8uXLNWPGDE2aNCly+913360xY8YoEAho7969evTRR9XU1KS33nqr3+9TXV2t1atXx7oGACBJxRygYDCoffv26YMPPoi6fcmSJZFfT548WUVFRZo9e7ZaWlo0bty4M75PVVWVKisrI1+Hw2EVFxfHuhYAIEnEFKBly5Zp+/bt2rlzp0aNGnXOx5aWlkqSmpub+w2Qz+eTz+eLZQ0AQBLzFCDnnB588EFt2bJFtbW1KikpOe/Mnj17JElFRUUxLQgASE2eAhQMBrVx40Zt27ZNWVlZ6ujokCT5/X5lZmaqpaVFGzdu1E9+8hONHDlSe/fu1YoVKzRz5kxNmTIlIf8DAADJyVOA1q1bJ+n0Xzb9pvXr12vx4sXKyMjQu+++qxdffFHd3d0qLi7WggUL9Pjjj8dtYQBAavD8R3DnUlxcrLq6ugtaCABwcUhz56vKAAuHw/L7/dZrAAAuUCgUUnZ29lnv52KkAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmBh0AXLOWa8AAIiD8/1+PugCdPToUesVAABxcL7fz9PcIHvL0dfXp7a2NmVlZSktLS3qvnA4rOLiYh08eFDZ2dlGG9rjPJzGeTiN83Aa5+G0wXAenHM6evSoAoGA0tPP/j5n6ADu9J2kp6dr1KhR53xMdnb2Rf0C+xrn4TTOw2mch9M4D6dZnwe/33/exwy6P4IDAFwcCBAAwERSBcjn82nVqlXy+XzWq5jiPJzGeTiN83Aa5+G0ZDoPg+5DCACAi0NSvQMCAKQOAgQAMEGAAAAmCBAAwETSBGjt2rW64oordMkll6i0tFQfffSR9UoD7sknn1RaWlrUMXHiROu1Em7nzp265ZZbFAgElJaWpq1bt0bd75zTypUrVVRUpMzMTJWXl2v//v02yybQ+c7D4sWLz3h9zJs3z2bZBKmurtb111+vrKws5efna/78+Wpqaop6zIkTJxQMBjVy5EiNGDFCCxYsUGdnp9HGifFdzsOsWbPOeD088MADRhv3LykC9Oabb6qyslKrVq3Sxx9/rKlTp2ru3Lk6fPiw9WoD7pprrlF7e3vk+OCDD6xXSrju7m5NnTpVa9eu7ff+NWvW6KWXXtLLL7+sXbt2afjw4Zo7d65OnDgxwJsm1vnOgyTNmzcv6vWxadOmAdww8erq6hQMBtXQ0KB33nlHp06d0pw5c9Td3R15zIoVK/T2229r8+bNqqurU1tbm26//XbDrePvu5wHSbrvvvuiXg9r1qwx2vgsXBKYPn26CwaDka97e3tdIBBw1dXVhlsNvFWrVrmpU6dar2FKktuyZUvk676+PldYWOiee+65yG1dXV3O5/O5TZs2GWw4ML59HpxzbtGiRe7WW2812cfK4cOHnSRXV1fnnDv9f/thw4a5zZs3Rx7z2WefOUmuvr7eas2E+/Z5cM65m266yf3617+2W+o7GPTvgE6ePKnGxkaVl5dHbktPT1d5ebnq6+sNN7Oxf/9+BQIBjR07VgsXLtSBAwesVzLV2tqqjo6OqNeH3+9XaWnpRfn6qK2tVX5+vq666iotXbpUR44csV4poUKhkCQpNzdXktTY2KhTp05FvR4mTpyo0aNHp/Tr4dvn4Wuvv/668vLyNGnSJFVVVen48eMW653VoLsY6bd9+eWX6u3tVUFBQdTtBQUF+vzzz422slFaWqoNGzboqquuUnt7u1avXq0bb7xR+/btU1ZWlvV6Jjo6OiSp39fH1/ddLObNm6fbb79dJSUlamlp0WOPPaaKigrV19dryJAh1uvFXV9fn5YvX64ZM2Zo0qRJkk6/HjIyMpSTkxP12FR+PfR3HiTp7rvv1pgxYxQIBLR37149+uijampq0ltvvWW4bbRBHyD8T0VFReTXU6ZMUWlpqcaMGaM///nPuvfeew03w2Bw5513Rn49efJkTZkyRePGjVNtba1mz55tuFliBINB7du376L4Oei5nO08LFmyJPLryZMnq6ioSLNnz1ZLS4vGjRs30Gv2a9D/EVxeXp6GDBlyxqdYOjs7VVhYaLTV4JCTk6MJEyaoubnZehUzX78GeH2caezYscrLy0vJ18eyZcu0fft2vf/++1H/fEthYaFOnjyprq6uqMen6uvhbOehP6WlpZI0qF4Pgz5AGRkZmjZtmmpqaiK39fX1qaamRmVlZYab2Tt27JhaWlpUVFRkvYqZkpISFRYWRr0+wuGwdu3addG/Pg4dOqQjR46k1OvDOadly5Zpy5Yteu+991RSUhJ1/7Rp0zRs2LCo10NTU5MOHDiQUq+H852H/uzZs0eSBtfrwfpTEN/FG2+84Xw+n9uwYYP79NNP3ZIlS1xOTo7r6OiwXm1APfTQQ662tta1tra6Dz/80JWXl7u8vDx3+PBh69US6ujRo+6TTz5xn3zyiZPknn/+effJJ5+4//znP84555599lmXk5Pjtm3b5vbu3etuvfVWV1JS4r766ivjzePrXOfh6NGj7uGHH3b19fWutbXVvfvuu+4HP/iBu/LKK92JEyesV4+bpUuXOr/f72pra117e3vkOH78eOQxDzzwgBs9erR777333O7du11ZWZkrKysz3Dr+zncempub3VNPPeV2797tWltb3bZt29zYsWPdzJkzjTePlhQBcs653//+92706NEuIyPDTZ8+3TU0NFivNODuuOMOV1RU5DIyMtz3vvc9d8cdd7jm5mbrtRLu/fffd5LOOBYtWuScO/1R7CeeeMIVFBQ4n8/nZs+e7ZqammyXToBznYfjx4+7OXPmuMsvv9wNGzbMjRkzxt13330p9/+k9fe/X5Jbv3595DFfffWV+9WvfuUuu+wyd+mll7rbbrvNtbe32y2dAOc7DwcOHHAzZ850ubm5zufzufHjx7vf/OY3LhQK2S7+LfxzDAAAE4P+Z0AAgNREgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJj4f0sJ0Pt3fif8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 9, Predicted label: 9\n"
     ]
    }
   ],
   "source": [
    "data_module = MNISTDataModule()\n",
    "# MNIST 테스트 데이터셋 로드\n",
    "predict_dataset = datasets.MNIST(\n",
    "    root='./', train=False, download=True)\n",
    "\n",
    "# 랜덤 이미지 선택\n",
    "random_idx = torch.randint(len(predict_dataset), size=(1,)).item()\n",
    "image, true_label = predict_dataset[random_idx]\n",
    "\n",
    "# 이미지 확인 (optional)\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "image_tensor = transform(image)\n",
    "plt.imshow(image_tensor[0].squeeze(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 모델 생성 및 학습된 가중치 로드\n",
    "model_config = {}\n",
    "model = ClassificationTask.load_from_checkpoint(\n",
    "    \"test_checkpoints/best_model.ckpt\", model=CNNModel(config=model_config))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = data_module.predict_instantly([image], [true_label])\n",
    "    loss, logits = model(x, y)\n",
    "\n",
    "# Predict Data를 원하는 DataLoader로 직접 만들어서 predict를 수행하고자 할 경우\n",
    "# data_module.predicted_dataloader_attr = your_dataloader\n",
    "# trainer = pl.Trainer()\n",
    "# loss, logits = trainer.predict(model, datamodule=data_module)\n",
    "\n",
    "\n",
    "# 가장 높은 확률을 가진 클래스 예측\n",
    "_, predicted_class = torch.max(logits, dim=1)\n",
    "\n",
    "print(f'True label: {true_label}, Predicted label: {predicted_class.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobVS_infra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
